{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Convnet #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matplotlib inline command allows us to see right below the code.\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import input_data\n",
    "import warnings # Ignore dumb warnings about deprecation I'll worry about this when I'm dead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Stuff for the plt figures\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trX.shape (55000, 784)\n",
      "trY.shape (55000, 10)\n",
      "teX.shape (10000, 784)\n",
      "teY.shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"trX.shape\", trX.shape)\n",
    "print(\"trY.shape\", trY.shape)\n",
    "print(\"teX.shape\", teX.shape)\n",
    "print(\"teY.shape\", teY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (44000, 784)\n",
      "X_val.shape (11000, 784)\n",
      "y_train.shape (44000, 10)\n",
      "y_val.shape (11000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Turn some into validation: We want 1/5th\n",
    "FRACTION_VAL = 5\n",
    "\n",
    "N,D = trX.shape\n",
    "seq = np.array(range(N))\n",
    "np.random.shuffle(seq)\n",
    "\n",
    "X_train = trX[seq[N/FRACTION_VAL:]]\n",
    "y_train = trY[seq[N/FRACTION_VAL:]]\n",
    "X_val = trX[seq[:N/FRACTION_VAL]]\n",
    "y_val = trY[seq[:N/FRACTION_VAL]]\n",
    "\n",
    "print(\"X_train.shape\" , X_train.shape)\n",
    "print(\"X_val.shape\" , X_val.shape)\n",
    "print(\"y_train.shape\" , y_train.shape)\n",
    "print(\"y_val.shape\" , y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels, in order:  [6 7 6 1 9 5 0 8 0 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAFzCAYAAAAjYj0YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAQAUkDQBABT4f+Cdp/5p546TAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d35e470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try to visualize some of these. The format of the images are 28x28.\n",
    "SHOW_IMAGE = 10\n",
    "\n",
    "for i in range(SHOW_IMAGE):\n",
    "    plt.subplot(SHOW_IMAGE/5, SHOW_IMAGE/2, i+1)\n",
    "    implot = plt.imshow(X_train[i].reshape((28,28)))\n",
    "    plt.axis('off')\n",
    "\n",
    "print(\"Labels, in order: \", np.where(y_train[:i+1] == 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from Stack Overflow, a batch normalizer.\n",
    "class ConvolutionalBatchNormalizer(object):\n",
    "    \"\"\"Helper class that groups the normalization logic and variables.        \n",
    "\n",
    "    Use:                                                                      \n",
    "      ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "      bn = ConvolutionalBatchNormalizer(depth, 0.001, ewma, True)           \n",
    "      update_assignments = bn.get_assigner()                                \n",
    "      x = bn.normalize(y, train=training?)                                  \n",
    "      (the output x will be batch-normalized).                              \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth, epsilon, ewma_trainer, scale_after_norm):\n",
    "        self.mean = tf.Variable(tf.constant(0.0, shape=[depth]),\n",
    "                                trainable=False)\n",
    "        self.variance = tf.Variable(tf.constant(1.0, shape=[depth]),\n",
    "                                    trainable=False)\n",
    "        self.beta = tf.Variable(tf.constant(0.0, shape=[depth]))\n",
    "        self.gamma = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "        self.ewma_trainer = ewma_trainer\n",
    "        self.epsilon = epsilon\n",
    "        self.scale_after_norm = scale_after_norm\n",
    "\n",
    "    def get_assigner(self):\n",
    "        \"\"\"Returns an EWMA apply op that must be invoked after optimization.\"\"\"\n",
    "        return self.ewma_trainer.apply([self.mean, self.variance])\n",
    "\n",
    "    def normalize(self, x, train=True):\n",
    "        \"\"\"Returns a batch-normalized version of x.\"\"\"\n",
    "        if train:\n",
    "            mean, variance = tf.nn.moments(x, [0, 1, 2])\n",
    "            assign_mean = self.mean.assign(mean)\n",
    "            assign_variance = self.variance.assign(variance)\n",
    "            with tf.control_dependencies([assign_mean, assign_variance]):\n",
    "                return tf.nn.batch_norm_with_global_normalization(\n",
    "                    x, mean, variance, self.beta, self.gamma,\n",
    "                    self.epsilon, self.scale_after_norm)\n",
    "        else:\n",
    "            mean = self.ewma_trainer.average(self.mean)\n",
    "            variance = self.ewma_trainer.average(self.variance)\n",
    "            local_beta = tf.identity(self.beta)\n",
    "            local_gamma = tf.identity(self.gamma)\n",
    "            return tf.nn.batch_norm_with_global_normalization(\n",
    "              x, mean, variance, local_beta, local_gamma,\n",
    "              self.epsilon, self.scale_after_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's build a neural network for this!\n",
    "# We have the architecture CONV -> RELU -> FC -> RELU -> FC -> SOFTMAX\n",
    "\n",
    "# Jupyter specific command\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def init_weight(shape, std = 0.01):\n",
    "    return tf.Variable(tf.random_normal(shape, std))\n",
    "\n",
    "def init_model(params):\n",
    "    X, W0, b0, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, conv_dropout, fc_dropout, train_batch = params\n",
    "    \n",
    "    ##### STARTING CONV #####\n",
    "    ## Layer 0 ##\n",
    "    # First we want to CONV the X to conv_1\n",
    "    conv_0 = tf.nn.conv2d(X, W0, [1,1,1,1], padding='SAME')\n",
    "    conv_0 = tf.nn.bias_add(conv_0, b0)\n",
    "    # Batch norm\n",
    "    ewma_0 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_0 = ConvolutionalBatchNormalizer(64, 0.001, ewma_0, True)           \n",
    "    update_assignments_0 = bn_0.get_assigner()                                \n",
    "    batch_0 = bn_0.normalize(conv_0, train=train_batch)\n",
    "    \n",
    "    relu_0 = tf.nn.relu(batch_0)\n",
    "    drop_0 = tf.nn.dropout(relu_0, conv_dropout)\n",
    "    \n",
    "    ## Layer 1 ##\n",
    "    # Then, we want to CONV the conv_1 to conv_2\n",
    "    conv_1 = tf.nn.conv2d(drop_0, W1, [1,1,1,1], padding='SAME')\n",
    "    conv_1 = tf.nn.bias_add(conv_1, b1)\n",
    "\n",
    "    # Batch norm\n",
    "    ewma_1 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_1 = ConvolutionalBatchNormalizer(64, 0.001, ewma_1, True)           \n",
    "    update_assignments_1 = bn_1.get_assigner()                                \n",
    "    batch_1 = bn_1.normalize(conv_1, train=train_batch)\n",
    "    relu_1 = tf.nn.relu(batch_1)\n",
    "    drop_1 = tf.nn.dropout(relu_1, conv_dropout)\n",
    "    \n",
    "    ## Layer 2 ##\n",
    "    # Then, we want to POOL the conv_2\n",
    "    pool_1 = tf.nn.max_pool(relu_1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # Then, we want to CONV the pool_2 to conv_3\n",
    "    conv_2 = tf.nn.conv2d(pool_1, W2, [1,1,1,1], padding='SAME')\n",
    "    conv_2 = tf.nn.bias_add(conv_2, b2)\n",
    "\n",
    "    # Batch norm\n",
    "    ewma_2 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_2 = ConvolutionalBatchNormalizer(128, 0.001, ewma_2, True)           \n",
    "    update_assignments_2 = bn_2.get_assigner()                                \n",
    "    batch_2 = bn_2.normalize(conv_2, train=train_batch)\n",
    "    \n",
    "    relu_2 = tf.nn.relu(batch_2)\n",
    "    drop_2 = tf.nn.dropout(relu_2, conv_dropout)\n",
    "    \n",
    "    ## Layer 3 ##\n",
    "    # Then, we want to POOL the conv_3\n",
    "    pool_2 = tf.nn.max_pool(relu_2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # Then, we want to CONV the pool_3\n",
    "    conv_3 = tf.nn.conv2d(pool_2, W3, [1,1,1,1], padding='SAME')\n",
    "    conv_3 = tf.nn.bias_add(conv_3, b3)\n",
    "\n",
    "    # Batch norm\n",
    "    ewma_3 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_3 = ConvolutionalBatchNormalizer(64, 0.001, ewma_3, True)           \n",
    "    update_assignments_3 = bn_3.get_assigner()                                \n",
    "    batch_3 = bn_3.normalize(conv_3, train=train_batch)\n",
    "    \n",
    "    relu_3 = tf.nn.relu(batch_3)\n",
    "    drop_3 = tf.nn.dropout(relu_3, conv_dropout)\n",
    "\n",
    "    \n",
    "    ##### STARTING FC #####\n",
    "    # Then we need to unroll this result and start FC\n",
    "    fc_3 = tf.reshape(drop_3, [-1, 7*7*64])\n",
    "    fc_4 = tf.matmul(fc_3, W4)\n",
    "    fc_4 += b4\n",
    "\n",
    "    # Batch norm\n",
    "#     ewma_4 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "#     bn_4 = ConvolutionalBatchNormalizer(1, 0.001, ewma_4, True)           \n",
    "#     update_assignments_4 = bn_4.get_assigner()                                \n",
    "#     batch_4 = bn_4.normalize(fc_4, train=train_batch)\n",
    "    \n",
    "    relu_4 = tf.nn.relu(fc_4)\n",
    "    drop_4 = tf.nn.dropout(relu_4, fc_dropout)\n",
    "    \n",
    "    # Then we need to fc again to get the result\n",
    "    fc_5 = tf.matmul(relu_4, W5)\n",
    "    fc_5 += b5\n",
    "\n",
    "    answer = fc_5\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable is a starting point. tf.random_normal initializes it for us with a stddev\n",
    "input_shape = X_train.shape\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "############################# CONVOLUTIONAL LAYER INITIALIZATION #############################\n",
    "# Each layer is in the form of [NxHxWxC]\n",
    "# We start off with a layer of [Nx28x28x1]\n",
    "# We change it to a layer of [Nx28x28x64] from CONV a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 1, out_channels = 64]\n",
    "W0 = init_weight([3,3,1,64])\n",
    "b0 = init_weight([64])\n",
    "# Then, we change it to a layer of [Nx28x28x64] from CONV a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 64, out_channels = 64\n",
    "W1 = init_weight([3,3,64,64])\n",
    "b1 = init_weight([64])\n",
    "\n",
    "# Then, we change it to a layer of [Nx14x14x64] from POOLING with filter of [2x2] with stride 2\n",
    "# No weights necessary.\n",
    "\n",
    "# Then, we change it to a layer of [Nx14x14x64] from CONV with a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 64, out_channels = 128]\n",
    "W2 = init_weight([3,3,64,128])\n",
    "b2 = init_weight([128])\n",
    "\n",
    "# Then, we get the layer of [Nx7x7x128] from POOLING with filter of [2x2] with stride 2\n",
    "# No weights necessary.\n",
    "\n",
    "# Then, we get the layer of [Nx7x7x64] from CONV with a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 128, out_channels = 64]\n",
    "W3 = init_weight([3,3,128,64])\n",
    "b3 = init_weight([64])\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "############################# FULLY CONNECTED LAYERS INITIALIZATION #############################\n",
    "# We need to unroll the parameters of the CONV layer, and we get an input of [Nx(7x7x64)]\n",
    "W4 = init_weight([7*7*64, 300])\n",
    "b4 = init_weight([1])\n",
    "\n",
    "# Then, we do one more FC before we feed into softmax:\n",
    "W5 = init_weight([300, 10])\n",
    "b5 = init_weight([1])\n",
    "################################################################################################\n",
    "\n",
    "#################################### EXTRA PARAMETERS ####################################\n",
    "conv_dropout = tf.placeholder(tf.float32)\n",
    "fc_dropout = tf.placeholder(tf.float32)\n",
    "train_batch = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparams:\n",
    "start_learning_rate = 5e-4\n",
    "reg_rate = 1e-3\n",
    "decay_steps = 100\n",
    "decay_rate = 0.90\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "\n",
    "hypothesis = init_model((X, W0, b0, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, conv_dropout, fc_dropout, train_batch))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y))\n",
    "\n",
    "# Regularization \n",
    "cross_entropy += reg_rate * (tf.nn.l2_loss(W0)+tf.nn.l2_loss(W1)+tf.nn.l2_loss(W2)\n",
    "                             +tf.nn.l2_loss(W3)+tf.nn.l2_loss(W4)+tf.nn.l2_loss(W5))\n",
    "\n",
    "# We want to decay the learning rate \n",
    "learning_rate = tf.train.exponential_decay(start_learning_rate, \n",
    "                                           global_step, \n",
    "                                           decay_steps, \n",
    "                                           decay_rate, \n",
    "                                           staircase=False, name=None)\n",
    "\n",
    "# Using Adam to optimize\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)\n",
    "predict_op = tf.argmax(hypothesis, 1)\n",
    "\n",
    "# Let's log it to see our progress!\n",
    "loss_summary = tf.scalar_summary('loss', cross_entropy)\n",
    "\n",
    "# Reshape the matrices into the correct conv dimensions:\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_val = X_val.reshape(-1,28,28,1)\n",
    "teX = teX.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting from model.ckpt checkpoint.\n",
      "Current learning_rate :  8.21649e-05\n",
      "Loss :  592.446\n",
      "0 0.92578125\n",
      "Current learning_rate :  8.13038e-05\n",
      "Loss :  601.813\n",
      "1 0.90625\n",
      "Current learning_rate :  8.04516e-05\n",
      "Loss :  602.022\n",
      "2 0.8984375\n",
      "Current learning_rate :  7.96084e-05\n",
      "Loss :  601.363\n",
      "3 0.90234375\n",
      "Current learning_rate :  7.87741e-05\n",
      "Loss :  578.776\n",
      "4 0.8828125\n",
      "Current learning_rate :  7.79485e-05\n",
      "Loss :  598.339\n",
      "5 0.91015625\n",
      "Current learning_rate :  7.71315e-05\n",
      "Loss :  609.278\n",
      "6 0.90625\n",
      "Current learning_rate :  7.63231e-05\n",
      "Loss :  604.299\n",
      "7 0.91015625\n",
      "Current learning_rate :  7.55232e-05\n",
      "Loss :  607.187\n",
      "8 0.91796875\n",
      "Current learning_rate :  7.47317e-05\n",
      "Loss :  597.244\n",
      "9 0.91796875\n",
      "Current learning_rate :  7.39484e-05\n",
      "Loss :  615.762\n",
      "10 0.8828125\n",
      "Current learning_rate :  7.31734e-05\n",
      "Loss :  627.298\n",
      "11 0.90625\n",
      "Current learning_rate :  7.24065e-05\n",
      "Loss :  603.052\n",
      "12 0.921875\n",
      "Current learning_rate :  7.16476e-05\n",
      "Loss :  629.119\n",
      "13 0.90625\n",
      "Current learning_rate :  7.08967e-05\n",
      "Loss :  579.253\n",
      "14 0.91796875\n",
      "Current learning_rate :  7.01536e-05\n",
      "Loss :  593.799\n",
      "15 0.91015625\n",
      "Current learning_rate :  6.94184e-05\n",
      "Loss :  607.917\n",
      "16 0.85546875\n",
      "Current learning_rate :  6.86908e-05\n",
      "Loss :  589.121\n",
      "17 0.890625\n",
      "Current learning_rate :  6.79709e-05\n",
      "Loss :  577.966\n",
      "18 0.92578125\n",
      "Current learning_rate :  6.72585e-05\n",
      "Loss :  606.907\n",
      "19 0.9375\n",
      "Current learning_rate :  6.65536e-05\n",
      "Loss :  606.791\n",
      "20 0.91015625\n",
      "Current learning_rate :  6.58561e-05\n",
      "Loss :  613.873\n",
      "21 0.93359375\n",
      "Current learning_rate :  6.51658e-05\n",
      "Loss :  595.765\n",
      "22 0.9140625\n",
      "Current learning_rate :  6.44828e-05\n",
      "Loss :  605.996\n",
      "23 0.9140625\n",
      "Current learning_rate :  6.3807e-05\n",
      "Loss :  592.64\n",
      "24 0.90625\n",
      "Current learning_rate :  6.31383e-05\n",
      "Loss :  575.911\n",
      "25 0.9140625\n",
      "Current learning_rate :  6.24765e-05\n",
      "Loss :  607.495\n",
      "26 0.9375\n",
      "Current learning_rate :  6.18217e-05\n",
      "Loss :  592.124\n",
      "27 0.90625\n",
      "Current learning_rate :  6.11738e-05\n",
      "Loss :  587.487\n",
      "28 0.921875\n",
      "Current learning_rate :  6.05326e-05\n",
      "Loss :  607.047\n",
      "29 0.9296875\n",
      "Current learning_rate :  5.98982e-05\n",
      "Loss :  605.461\n",
      "30 0.9453125\n",
      "Current learning_rate :  5.92704e-05\n",
      "Loss :  580.89\n",
      "31 0.90625\n",
      "Current learning_rate :  5.86492e-05\n",
      "Loss :  622.545\n",
      "32 0.87890625\n",
      "Current learning_rate :  5.80346e-05\n",
      "Loss :  587.257\n",
      "33 0.8984375\n",
      "Current learning_rate :  5.74263e-05\n",
      "Loss :  614.53\n",
      "34 0.90625\n",
      "Current learning_rate :  5.68244e-05\n",
      "Loss :  617.113\n",
      "35 0.91796875\n",
      "Current learning_rate :  5.62289e-05\n",
      "Loss :  616.348\n",
      "36 0.9296875\n",
      "Current learning_rate :  5.56396e-05\n",
      "Loss :  580.898\n",
      "37 0.93359375\n",
      "Current learning_rate :  5.50564e-05\n",
      "Loss :  597.506\n",
      "38 0.91796875\n",
      "Current learning_rate :  5.44794e-05\n",
      "Loss :  583.345\n",
      "39 0.90625\n",
      "Current learning_rate :  5.39084e-05\n",
      "Loss :  605.307\n",
      "40 0.921875\n",
      "Current learning_rate :  5.33434e-05\n",
      "Loss :  586.71\n",
      "41 0.921875\n",
      "Current learning_rate :  5.27843e-05\n",
      "Loss :  599.041\n",
      "42 0.94921875\n",
      "Current learning_rate :  5.22311e-05\n",
      "Loss :  602.396\n",
      "43 0.90625\n",
      "Current learning_rate :  5.16837e-05\n",
      "Loss :  609.933\n",
      "44 0.92578125\n",
      "Current learning_rate :  5.1142e-05\n",
      "Loss :  616.087\n",
      "45 0.875\n",
      "Current learning_rate :  5.0606e-05\n",
      "Loss :  590.254\n",
      "46 0.89453125\n",
      "Current learning_rate :  5.00756e-05\n",
      "Loss :  624.499\n",
      "47 0.890625\n",
      "Current learning_rate :  4.95508e-05\n",
      "Loss :  581.201\n",
      "48 0.921875\n",
      "Current learning_rate :  4.90314e-05\n",
      "Loss :  612.527\n",
      "49 0.94921875\n",
      "Current learning_rate :  4.85176e-05\n",
      "Loss :  587.727\n",
      "50 0.9140625\n",
      "Current learning_rate :  4.80091e-05\n",
      "Loss :  612.918\n",
      "51 0.91015625\n",
      "Current learning_rate :  4.75059e-05\n",
      "Loss :  602.424\n",
      "52 0.9296875\n",
      "Current learning_rate :  4.7008e-05\n",
      "Loss :  596.623\n",
      "53 0.9140625\n",
      "Current learning_rate :  4.65153e-05\n",
      "Loss :  608.133\n",
      "54 0.9296875\n",
      "Current learning_rate :  4.60278e-05\n",
      "Loss :  607.759\n",
      "55 0.9140625\n",
      "Current learning_rate :  4.55454e-05\n",
      "Loss :  614.999\n",
      "56 0.921875\n",
      "Current learning_rate :  4.5068e-05\n",
      "Loss :  587.417\n",
      "57 0.90234375\n",
      "Current learning_rate :  4.45957e-05\n",
      "Loss :  608.175\n",
      "58 0.91796875\n",
      "Current learning_rate :  4.41283e-05\n",
      "Loss :  576.23\n",
      "59 0.88671875\n",
      "Current learning_rate :  4.36658e-05\n",
      "Loss :  585.789\n",
      "60 0.8984375\n",
      "Current learning_rate :  4.32082e-05\n",
      "Loss :  602.506\n",
      "61 0.9140625\n",
      "Current learning_rate :  4.27553e-05\n",
      "Loss :  596.185\n",
      "62 0.91015625\n",
      "Current learning_rate :  4.23072e-05\n",
      "Loss :  601.404\n",
      "63 0.9453125\n",
      "Current learning_rate :  4.18638e-05\n",
      "Loss :  610.433\n",
      "64 0.9140625\n",
      "Current learning_rate :  4.1425e-05\n",
      "Loss :  587.038\n",
      "65 0.90234375\n",
      "Current learning_rate :  4.09908e-05\n",
      "Loss :  612.08\n",
      "66 0.8828125\n",
      "Current learning_rate :  4.05612e-05\n",
      "Loss :  600.748\n",
      "67 0.8984375\n",
      "Current learning_rate :  4.01361e-05\n",
      "Loss :  603.378\n",
      "68 0.94921875\n",
      "Current learning_rate :  3.97155e-05\n",
      "Loss :  590.88\n",
      "69 0.9296875\n",
      "Current learning_rate :  3.92992e-05\n",
      "Loss :  587.02\n",
      "70 0.90234375\n",
      "Current learning_rate :  3.88873e-05\n",
      "Loss :  600.358\n",
      "71 0.90234375\n",
      "Current learning_rate :  3.84798e-05\n",
      "Loss :  612.759\n",
      "72 0.88671875\n",
      "Current learning_rate :  3.80765e-05\n",
      "Loss :  622.303\n",
      "73 0.94921875\n",
      "Current learning_rate :  3.76774e-05\n",
      "Loss :  603.544\n",
      "74 0.9375\n",
      "Current learning_rate :  3.72825e-05\n",
      "Loss :  631.059\n",
      "75 0.890625\n",
      "Current learning_rate :  3.68918e-05\n",
      "Loss :  580.24\n",
      "76 0.890625\n",
      "Current learning_rate :  3.65051e-05\n",
      "Loss :  564.842\n",
      "77 0.91796875\n",
      "Current learning_rate :  3.61225e-05\n",
      "Loss :  602.803\n",
      "78 0.9296875\n",
      "Current learning_rate :  3.57439e-05\n",
      "Loss :  589.896\n",
      "79 0.91015625\n",
      "Current learning_rate :  3.53693e-05\n",
      "Loss :  570.466\n",
      "80 0.90625\n",
      "Current learning_rate :  3.49986e-05\n",
      "Loss :  597.002\n",
      "81 0.8828125\n",
      "Current learning_rate :  3.46318e-05\n",
      "Loss :  595.005\n",
      "82 0.89453125\n",
      "Current learning_rate :  3.42688e-05\n",
      "Loss :  581.264\n",
      "83 0.91015625\n",
      "Current learning_rate :  3.39097e-05\n",
      "Loss :  612.009\n",
      "84 0.92578125\n",
      "Current learning_rate :  3.35543e-05\n",
      "Loss :  617.29\n",
      "85 0.91796875\n",
      "Current learning_rate :  3.32026e-05\n",
      "Loss :  597.514\n",
      "86 0.859375\n",
      "Current learning_rate :  3.28546e-05\n",
      "Loss :  577.507\n",
      "87 0.9140625\n",
      "Current learning_rate :  3.25103e-05\n",
      "Loss :  602.109\n",
      "88 0.93359375\n",
      "Current learning_rate :  3.21695e-05\n",
      "Loss :  615.592\n",
      "89 0.90625\n",
      "Current learning_rate :  3.18324e-05\n",
      "Loss :  595.03\n",
      "90 0.90234375\n",
      "Current learning_rate :  3.14987e-05\n",
      "Loss :  583.408\n",
      "91 0.91015625\n",
      "Current learning_rate :  3.11686e-05\n",
      "Loss :  610.628\n",
      "92 0.95703125\n",
      "Current learning_rate :  3.08419e-05\n",
      "Loss :  598.443\n",
      "93 0.9453125\n",
      "Current learning_rate :  3.05187e-05\n",
      "Loss :  595.227\n",
      "94 0.94140625\n",
      "Current learning_rate :  3.01988e-05\n",
      "Loss :  596.824\n",
      "95 0.91796875\n",
      "Current learning_rate :  2.98823e-05\n",
      "Loss :  593.766\n",
      "96 0.92578125\n",
      "Current learning_rate :  2.95691e-05\n",
      "Loss :  594.105\n",
      "97 0.93359375\n",
      "Current learning_rate :  2.92592e-05\n",
      "Loss :  580.559\n",
      "98 0.92578125\n",
      "Current learning_rate :  2.89526e-05\n",
      "Loss :  586.421\n",
      "99 0.953125\n",
      "Model saved in file: model.ckpt\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # If we already have the model saved\n",
    "    try:\n",
    "        saver.restore(sess, \"model.ckpt\")\n",
    "        print(\"Starting from model.ckpt checkpoint.\")\n",
    "    except:\n",
    "        print(\"Was unable to read model.ckpt checkpoint.\")\n",
    "        # you need to initialize all variables\n",
    "        init_op = tf.initialize_all_variables()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "    writer = tf.train.SummaryWriter('logs', sess.graph_def)\n",
    "    \n",
    "    try:    \n",
    "        for i in range(100):\n",
    "            NUM_BATCHES = 10\n",
    "            for j in range(NUM_BATCHES):\n",
    "                batch_mask = np.random.choice(44000, 128)\n",
    "                summary_str, _, cur_loss = sess.run([loss_summary, train_op, cross_entropy], \n",
    "                                                    feed_dict={X: X_train[batch_mask], \n",
    "                                                               y: y_train[batch_mask],\n",
    "                                                               conv_dropout: 0.5, \n",
    "                                                               fc_dropout: 0.5,\n",
    "                                                               train_batch: True})\n",
    "            print(\"Current learning_rate : \", learning_rate.eval())\n",
    "            test_indices = np.arange(len(X_val)) # Get a validation batch\n",
    "            np.random.shuffle(test_indices)\n",
    "            test_indices = test_indices[0:256]\n",
    "\n",
    "            writer.add_summary(summary_str, i)\n",
    "            print(\"Loss : \" , cur_loss)\n",
    "            print(i, np.mean(np.argmax(y_val[test_indices], axis=1) ==\n",
    "                             sess.run(predict_op, feed_dict={X: X_val[test_indices],\n",
    "                                                             y: y_val[test_indices],\n",
    "                                                             conv_dropout: 1.0,\n",
    "                                                             fc_dropout: 1.0,\n",
    "                                                             train_batch: False})))\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted')\n",
    "        save_path = saver.save(sess, \"model.ckpt\") # checkpoint file\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n",
    "    save_path = saver.save(sess, \"model.ckpt\") # checkpoint file\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answers :  [7 2 1 0 4 1 4 9 5 9]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAFzCAYAAAAjYj0YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x119715080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"model.ckpt\")\n",
    "    answers = sess.run(predict_op, feed_dict={X: teX[:SHOW_IMAGE],\n",
    "                                     y: teY[:SHOW_IMAGE],\n",
    "                                     conv_dropout: 1.0,\n",
    "                                     fc_dropout: 1.0})\n",
    "    for i in range(SHOW_IMAGE):\n",
    "        plt.subplot(SHOW_IMAGE/5, SHOW_IMAGE/2, i+1)\n",
    "        implot = plt.imshow(teX[i].reshape((28,28)))\n",
    "        plt.axis('off')\n",
    "    print(\"Answers : \", answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
