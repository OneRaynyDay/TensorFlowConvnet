{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Convnet #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# matplotlib inline command allows us to see right below the code.\n",
    "%matplotlib inline\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import input_data\n",
    "import warnings # Ignore dumb warnings about deprecation I'll worry about this when I'm dead\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Stuff for the plt figures\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "trX, trY, teX, teY = mnist.train.images, mnist.train.labels, mnist.test.images, mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trX.shape (55000, 784)\n",
      "trY.shape (55000, 10)\n",
      "teX.shape (10000, 784)\n",
      "teY.shape (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(\"trX.shape\", trX.shape)\n",
    "print(\"trY.shape\", trY.shape)\n",
    "print(\"teX.shape\", teX.shape)\n",
    "print(\"teY.shape\", teY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (44000, 784)\n",
      "X_val.shape (11000, 784)\n",
      "y_train.shape (44000, 10)\n",
      "y_val.shape (11000, 10)\n"
     ]
    }
   ],
   "source": [
    "# Turn some into validation: We want 1/5th\n",
    "FRACTION_VAL = 5\n",
    "\n",
    "N,D = trX.shape\n",
    "seq = np.array(range(N))\n",
    "np.random.shuffle(seq)\n",
    "\n",
    "X_train = trX[seq[N/FRACTION_VAL:]]\n",
    "y_train = trY[seq[N/FRACTION_VAL:]]\n",
    "X_val = trX[seq[:N/FRACTION_VAL]]\n",
    "y_val = trY[seq[:N/FRACTION_VAL]]\n",
    "\n",
    "print(\"X_train.shape\" , X_train.shape)\n",
    "print(\"X_val.shape\" , X_val.shape)\n",
    "print(\"y_train.shape\" , y_train.shape)\n",
    "print(\"y_val.shape\" , y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels, in order:  [7 4 5 3 9 2 4 2 7 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAFzCAYAAAAjYj0YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naAIAyPB/gVbI5vz4DkoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d455470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's try to visualize some of these. The format of the images are 28x28.\n",
    "SHOW_IMAGE = 10\n",
    "\n",
    "for i in range(SHOW_IMAGE):\n",
    "    plt.subplot(SHOW_IMAGE/5, SHOW_IMAGE/2, i+1)\n",
    "    implot = plt.imshow(X_train[i].reshape((28,28)))\n",
    "    plt.axis('off')\n",
    "\n",
    "print(\"Labels, in order: \", np.where(y_train[:i+1] == 1)[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Taken from Stack Overflow, a batch normalizer.\n",
    "class ConvolutionalBatchNormalizer(object):\n",
    "    \"\"\"Helper class that groups the normalization logic and variables.        \n",
    "\n",
    "    Use:                                                                      \n",
    "      ewma = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "      bn = ConvolutionalBatchNormalizer(depth, 0.001, ewma, True)           \n",
    "      update_assignments = bn.get_assigner()                                \n",
    "      x = bn.normalize(y, train=training?)                                  \n",
    "      (the output x will be batch-normalized).                              \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, depth, epsilon, ewma_trainer, scale_after_norm):\n",
    "        self.mean = tf.Variable(tf.constant(0.0, shape=[depth]),\n",
    "                                trainable=False)\n",
    "        self.variance = tf.Variable(tf.constant(1.0, shape=[depth]),\n",
    "                                    trainable=False)\n",
    "        self.beta = tf.Variable(tf.constant(0.0, shape=[depth]))\n",
    "        self.gamma = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "        self.ewma_trainer = ewma_trainer\n",
    "        self.epsilon = epsilon\n",
    "        self.scale_after_norm = scale_after_norm\n",
    "\n",
    "    def get_assigner(self):\n",
    "        \"\"\"Returns an EWMA apply op that must be invoked after optimization.\"\"\"\n",
    "        return self.ewma_trainer.apply([self.mean, self.variance])\n",
    "\n",
    "    def normalize(self, x, train=True):\n",
    "        \"\"\"Returns a batch-normalized version of x.\"\"\"\n",
    "        if train:\n",
    "            mean, variance = tf.nn.moments(x, [0, 1, 2])\n",
    "            assign_mean = self.mean.assign(mean)\n",
    "            assign_variance = self.variance.assign(variance)\n",
    "            with tf.control_dependencies([assign_mean, assign_variance]):\n",
    "                return tf.nn.batch_norm_with_global_normalization(\n",
    "                    x, mean, variance, self.beta, self.gamma,\n",
    "                    self.epsilon, self.scale_after_norm)\n",
    "        else:\n",
    "            mean = self.ewma_trainer.average(self.mean)\n",
    "            variance = self.ewma_trainer.average(self.variance)\n",
    "            local_beta = tf.identity(self.beta)\n",
    "            local_gamma = tf.identity(self.gamma)\n",
    "            return tf.nn.batch_norm_with_global_normalization(\n",
    "              x, mean, variance, local_beta, local_gamma,\n",
    "              self.epsilon, self.scale_after_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's build a neural network for this!\n",
    "# We have the architecture CONV -> RELU -> FC -> RELU -> FC -> SOFTMAX\n",
    "\n",
    "# Jupyter specific command\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def init_weight(shape, std = 0.01):\n",
    "    return tf.Variable(tf.random_normal(shape, std))\n",
    "\n",
    "def init_model(params):\n",
    "    X, W0, b0, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, conv_dropout, fc_dropout, train_batch = params\n",
    "    \n",
    "    ##### STARTING CONV #####\n",
    "    ## Layer 0 ##\n",
    "    # First we want to CONV the X to conv_1\n",
    "    conv_0 = tf.nn.conv2d(X, W0, [1,1,1,1], padding='SAME')\n",
    "    conv_0 = tf.nn.bias_add(conv_0, b0)\n",
    "    # Batch norm\n",
    "    ewma_0 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_0 = ConvolutionalBatchNormalizer(64, 0.001, ewma_0, True)           \n",
    "    update_assignments_0 = bn_0.get_assigner()                                \n",
    "    batch_0 = bn_0.normalize(conv_0, train=train_batch)\n",
    "    \n",
    "    relu_0 = tf.nn.relu(batch_0)\n",
    "    drop_0 = tf.nn.dropout(relu_0, conv_dropout)\n",
    "    \n",
    "    ## Layer 1 ##\n",
    "    # Then, we want to CONV the conv_1 to conv_2\n",
    "    conv_1 = tf.nn.conv2d(drop_0, W1, [1,1,1,1], padding='SAME')\n",
    "    conv_1 = tf.nn.bias_add(conv_1, b1)\n",
    "\n",
    "    # Batch norm\n",
    "    ewma_1 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_1 = ConvolutionalBatchNormalizer(64, 0.001, ewma_1, True)           \n",
    "    update_assignments_1 = bn_1.get_assigner()                                \n",
    "    batch_1 = bn_1.normalize(conv_1, train=train_batch)\n",
    "    relu_1 = tf.nn.relu(batch_1)\n",
    "    drop_1 = tf.nn.dropout(relu_1, conv_dropout)\n",
    "    \n",
    "    ## Layer 2 ##\n",
    "    # Then, we want to POOL the conv_2\n",
    "    pool_1 = tf.nn.max_pool(relu_1, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # Then, we want to CONV the pool_2 to conv_3\n",
    "    conv_2 = tf.nn.conv2d(pool_1, W2, [1,1,1,1], padding='SAME')\n",
    "    conv_2 = tf.nn.bias_add(conv_2, b2)\n",
    "\n",
    "    # Batch norm\n",
    "    ewma_2 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_2 = ConvolutionalBatchNormalizer(128, 0.001, ewma_2, True)           \n",
    "    update_assignments_2 = bn_2.get_assigner()                                \n",
    "    batch_2 = bn_2.normalize(conv_2, train=train_batch)\n",
    "    \n",
    "    relu_2 = tf.nn.relu(batch_2)\n",
    "    drop_2 = tf.nn.dropout(relu_2, conv_dropout)\n",
    "    \n",
    "    ## Layer 3 ##\n",
    "    # Then, we want to POOL the conv_3\n",
    "    pool_2 = tf.nn.max_pool(relu_2, [1,2,2,1], [1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # Then, we want to CONV the pool_3\n",
    "    conv_3 = tf.nn.conv2d(pool_2, W3, [1,1,1,1], padding='SAME')\n",
    "    conv_3 = tf.nn.bias_add(conv_3, b3)\n",
    "\n",
    "    # Batch norm\n",
    "    ewma_3 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "    bn_3 = ConvolutionalBatchNormalizer(64, 0.001, ewma_3, True)           \n",
    "    update_assignments_3 = bn_3.get_assigner()                                \n",
    "    batch_3 = bn_3.normalize(conv_3, train=train_batch)\n",
    "    \n",
    "    relu_3 = tf.nn.relu(batch_3)\n",
    "    drop_3 = tf.nn.dropout(relu_3, conv_dropout)\n",
    "\n",
    "    \n",
    "    ##### STARTING FC #####\n",
    "    # Then we need to unroll this result and start FC\n",
    "    fc_3 = tf.reshape(drop_3, [-1, 7*7*64])\n",
    "    fc_4 = tf.matmul(fc_3, W4)\n",
    "    fc_4 += b4\n",
    "\n",
    "    # Batch norm\n",
    "#     ewma_4 = tf.train.ExponentialMovingAverage(decay=0.99)                  \n",
    "#     bn_4 = ConvolutionalBatchNormalizer(1, 0.001, ewma_4, True)           \n",
    "#     update_assignments_4 = bn_4.get_assigner()                                \n",
    "#     batch_4 = bn_4.normalize(fc_4, train=train_batch)\n",
    "    \n",
    "    relu_4 = tf.nn.relu(fc_4)\n",
    "    drop_4 = tf.nn.dropout(relu_4, fc_dropout)\n",
    "    \n",
    "    # Then we need to fc again to get the result\n",
    "    fc_5 = tf.matmul(relu_4, W5)\n",
    "    fc_5 += b5\n",
    "\n",
    "    answer = fc_5\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Variable is a starting point. tf.random_normal initializes it for us with a stddev\n",
    "input_shape = X_train.shape\n",
    "X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "############################# CONVOLUTIONAL LAYER INITIALIZATION #############################\n",
    "# Each layer is in the form of [NxHxWxC]\n",
    "# We start off with a layer of [Nx28x28x1]\n",
    "# We change it to a layer of [Nx28x28x64] from CONV a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 1, out_channels = 64]\n",
    "W0 = init_weight([3,3,1,64])\n",
    "b0 = init_weight([64])\n",
    "# Then, we change it to a layer of [Nx28x28x64] from CONV a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 64, out_channels = 64\n",
    "W1 = init_weight([3,3,64,64])\n",
    "b1 = init_weight([64])\n",
    "\n",
    "# Then, we change it to a layer of [Nx14x14x64] from POOLING with filter of [2x2] with stride 2\n",
    "# No weights necessary.\n",
    "\n",
    "# Then, we change it to a layer of [Nx14x14x64] from CONV with a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 64, out_channels = 128]\n",
    "W2 = init_weight([3,3,64,128])\n",
    "b2 = init_weight([128])\n",
    "\n",
    "# Then, we get the layer of [Nx7x7x128] from POOLING with filter of [2x2] with stride 2\n",
    "# No weights necessary.\n",
    "\n",
    "# Then, we get the layer of [Nx7x7x64] from CONV with a filter of [3x3], padding 1\n",
    "# Therefore, we need the weight [filter_height = 3, filter_width = 3, in_channels = 128, out_channels = 64]\n",
    "W3 = init_weight([3,3,128,64])\n",
    "b3 = init_weight([64])\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "############################# FULLY CONNECTED LAYERS INITIALIZATION #############################\n",
    "# We need to unroll the parameters of the CONV layer, and we get an input of [Nx(7x7x64)]\n",
    "W4 = init_weight([7*7*64, 300])\n",
    "b4 = init_weight([1])\n",
    "\n",
    "# Then, we do one more FC before we feed into softmax:\n",
    "W5 = init_weight([300, 10])\n",
    "b5 = init_weight([1])\n",
    "################################################################################################\n",
    "\n",
    "#################################### EXTRA PARAMETERS ####################################\n",
    "conv_dropout = tf.placeholder(tf.float32)\n",
    "fc_dropout = tf.placeholder(tf.float32)\n",
    "train_batch = tf.placeholder(tf.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hyperparams:\n",
    "start_learning_rate = 5e-4\n",
    "reg_rate = 1e-3\n",
    "decay_steps = 100\n",
    "decay_rate = 0.90\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "\n",
    "\n",
    "hypothesis = init_model((X, W0, b0, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5, conv_dropout, fc_dropout, train_batch))\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(hypothesis, y))\n",
    "\n",
    "# Regularization \n",
    "cross_entropy += reg_rate * (tf.nn.l2_loss(W0)+tf.nn.l2_loss(W1)+tf.nn.l2_loss(W2)\n",
    "                             +tf.nn.l2_loss(W3)+tf.nn.l2_loss(W4)+tf.nn.l2_loss(W5))\n",
    "\n",
    "# We want to decay the learning rate \n",
    "learning_rate = tf.train.exponential_decay(start_learning_rate, \n",
    "                                           global_step, \n",
    "                                           decay_steps, \n",
    "                                           decay_rate, \n",
    "                                           staircase=False, name=None)\n",
    "\n",
    "# Using Adam to optimize\n",
    "train_op = tf.train.AdamOptimizer(learning_rate).minimize(cross_entropy, global_step=global_step)\n",
    "predict_op = tf.argmax(hypothesis, 1)\n",
    "\n",
    "# Let's log it to see our progress!\n",
    "loss_summary = tf.scalar_summary('loss', cross_entropy)\n",
    "\n",
    "# Reshape the matrices into the correct conv dimensions:\n",
    "X_train = X_train.reshape(-1,28,28,1)\n",
    "X_val = X_val.reshape(-1,28,28,1)\n",
    "teX = teX.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Was unable to read best_model.ckpt checkpoint. max_acc = 0\n",
      "Starting from model.ckpt checkpoint.\n",
      "Current learning_rate :  1.18236e-05\n",
      "Loss :  564.883\n",
      "0 0.89453125\n",
      "Best model saved in file: best_model.ckpt\n",
      "Current learning_rate :  1.16996e-05\n",
      "Loss :  598.741\n",
      "1 0.92578125\n",
      "Best model saved in file: best_model.ckpt\n",
      "Current learning_rate :  1.1577e-05\n",
      "Loss :  605.087\n",
      "2 0.91796875\n",
      "Current learning_rate :  1.14557e-05\n",
      "Loss :  573.351\n",
      "3 0.94140625\n",
      "Best model saved in file: best_model.ckpt\n",
      "Current learning_rate :  1.13356e-05\n",
      "Loss :  585.392\n",
      "4 0.91796875\n",
      "Current learning_rate :  1.12168e-05\n",
      "Loss :  581.429\n",
      "5 0.91796875\n",
      "Current learning_rate :  1.10993e-05\n",
      "Loss :  582.311\n",
      "6 0.91015625\n",
      "Current learning_rate :  1.09829e-05\n",
      "Loss :  616.586\n",
      "7 0.921875\n",
      "Current learning_rate :  1.08678e-05\n",
      "Loss :  582.129\n",
      "8 0.91796875\n",
      "Current learning_rate :  1.07539e-05\n",
      "Loss :  567.618\n",
      "9 0.90625\n",
      "Current learning_rate :  1.06412e-05\n",
      "Loss :  620.567\n",
      "10 0.90625\n",
      "Current learning_rate :  1.05297e-05\n",
      "Loss :  600.931\n",
      "11 0.91015625\n",
      "Current learning_rate :  1.04193e-05\n",
      "Loss :  567.161\n",
      "12 0.9375\n",
      "Current learning_rate :  1.03101e-05\n",
      "Loss :  600.994\n",
      "13 0.9296875\n",
      "Current learning_rate :  1.02021e-05\n",
      "Loss :  612.052\n",
      "14 0.90625\n",
      "Current learning_rate :  1.00951e-05\n",
      "Loss :  584.185\n",
      "15 0.890625\n",
      "Current learning_rate :  9.98933e-06\n",
      "Loss :  601.24\n",
      "16 0.93359375\n",
      "Current learning_rate :  9.88463e-06\n",
      "Loss :  613.52\n",
      "17 0.8828125\n",
      "Current learning_rate :  9.78104e-06\n",
      "Loss :  564.545\n",
      "18 0.9296875\n",
      "Current learning_rate :  9.67853e-06\n",
      "Loss :  589.087\n",
      "19 0.9375\n",
      "Current learning_rate :  9.57709e-06\n",
      "Loss :  594.054\n",
      "20 0.90625\n",
      "Current learning_rate :  9.47671e-06\n",
      "Loss :  582.806\n",
      "21 0.92578125\n",
      "Current learning_rate :  9.37739e-06\n",
      "Loss :  595.693\n",
      "22 0.9375\n",
      "Current learning_rate :  9.27911e-06\n",
      "Loss :  586.775\n",
      "23 0.9296875\n",
      "Current learning_rate :  9.18186e-06\n",
      "Loss :  585.342\n",
      "24 0.8828125\n",
      "Current learning_rate :  9.08562e-06\n",
      "Loss :  581.287\n",
      "25 0.89453125\n",
      "Current learning_rate :  8.9904e-06\n",
      "Loss :  599.057\n",
      "26 0.8984375\n",
      "Current learning_rate :  8.89617e-06\n",
      "Loss :  582.509\n",
      "27 0.9375\n",
      "Current learning_rate :  8.80293e-06\n",
      "Loss :  585.431\n",
      "28 0.921875\n",
      "Current learning_rate :  8.71067e-06\n",
      "Loss :  600.524\n",
      "29 0.8984375\n",
      "Current learning_rate :  8.61938e-06\n",
      "Loss :  618.451\n",
      "30 0.88671875\n",
      "Current learning_rate :  8.52904e-06\n",
      "Loss :  585.084\n",
      "31 0.90234375\n",
      "Current learning_rate :  8.43965e-06\n",
      "Loss :  601.745\n",
      "32 0.91015625\n",
      "Current learning_rate :  8.35119e-06\n",
      "Loss :  602.695\n",
      "33 0.890625\n",
      "Current learning_rate :  8.26367e-06\n",
      "Loss :  598.085\n",
      "34 0.921875\n",
      "Current learning_rate :  8.17706e-06\n",
      "Loss :  576.443\n",
      "35 0.92578125\n",
      "Current learning_rate :  8.09136e-06\n",
      "Loss :  598.293\n",
      "36 0.921875\n",
      "Current learning_rate :  8.00655e-06\n",
      "Loss :  580.067\n",
      "37 0.8984375\n",
      "Current learning_rate :  7.92264e-06\n",
      "Loss :  597.031\n",
      "38 0.90234375\n",
      "Current learning_rate :  7.83961e-06\n",
      "Loss :  605.251\n",
      "39 0.95703125\n",
      "Best model saved in file: best_model.ckpt\n",
      "Current learning_rate :  7.75744e-06\n",
      "Loss :  595.944\n",
      "40 0.92578125\n",
      "Current learning_rate :  7.67614e-06\n",
      "Loss :  559.291\n",
      "41 0.9140625\n",
      "Current learning_rate :  7.59568e-06\n",
      "Loss :  615.838\n",
      "42 0.9375\n",
      "Current learning_rate :  7.51608e-06\n",
      "Loss :  563.883\n",
      "43 0.90234375\n",
      "Current learning_rate :  7.4373e-06\n",
      "Loss :  573.729\n",
      "44 0.87109375\n",
      "Current learning_rate :  7.35935e-06\n",
      "Loss :  569.487\n",
      "45 0.89453125\n",
      "Current learning_rate :  7.28222e-06\n",
      "Loss :  593.996\n",
      "46 0.9140625\n",
      "Current learning_rate :  7.2059e-06\n",
      "Loss :  603.944\n",
      "47 0.90625\n",
      "Current learning_rate :  7.13037e-06\n",
      "Loss :  596.739\n",
      "48 0.91015625\n",
      "Current learning_rate :  7.05564e-06\n",
      "Loss :  579.954\n",
      "49 0.93359375\n",
      "Current learning_rate :  6.98169e-06\n",
      "Loss :  586.154\n",
      "50 0.93359375\n",
      "Current learning_rate :  6.90852e-06\n",
      "Loss :  591.976\n",
      "51 0.9140625\n",
      "Current learning_rate :  6.83611e-06\n",
      "Loss :  554.823\n",
      "52 0.9609375\n",
      "Best model saved in file: best_model.ckpt\n",
      "Current learning_rate :  6.76447e-06\n",
      "Loss :  567.832\n",
      "53 0.94140625\n",
      "Current learning_rate :  6.69357e-06\n",
      "Loss :  583.023\n",
      "54 0.9140625\n",
      "Current learning_rate :  6.62342e-06\n",
      "Loss :  599.924\n",
      "55 0.92578125\n",
      "Current learning_rate :  6.554e-06\n",
      "Loss :  580.868\n",
      "56 0.8828125\n",
      "Current learning_rate :  6.48531e-06\n",
      "Loss :  613.627\n",
      "57 0.9453125\n",
      "Current learning_rate :  6.41734e-06\n",
      "Loss :  584.305\n",
      "58 0.91796875\n",
      "Current learning_rate :  6.35008e-06\n",
      "Loss :  609.041\n",
      "59 0.93359375\n",
      "Current learning_rate :  6.28353e-06\n",
      "Loss :  572.996\n",
      "60 0.90234375\n",
      "Current learning_rate :  6.21767e-06\n",
      "Loss :  577.946\n",
      "61 0.921875\n",
      "Current learning_rate :  6.1525e-06\n",
      "Loss :  603.432\n",
      "62 0.91796875\n",
      "Current learning_rate :  6.08802e-06\n",
      "Loss :  597.241\n",
      "63 0.90625\n",
      "Current learning_rate :  6.02421e-06\n",
      "Loss :  610.64\n",
      "64 0.91796875\n",
      "Current learning_rate :  5.96108e-06\n",
      "Loss :  607.933\n",
      "65 0.9140625\n",
      "Current learning_rate :  5.8986e-06\n",
      "Loss :  608.765\n",
      "66 0.921875\n",
      "Current learning_rate :  5.83678e-06\n",
      "Loss :  587.579\n",
      "67 0.92578125\n",
      "Current learning_rate :  5.7756e-06\n",
      "Loss :  611.42\n",
      "68 0.91796875\n",
      "Current learning_rate :  5.71507e-06\n",
      "Loss :  610.168\n",
      "69 0.91796875\n",
      "Current learning_rate :  5.65517e-06\n",
      "Loss :  587.6\n",
      "70 0.921875\n",
      "Current learning_rate :  5.5959e-06\n",
      "Loss :  569.454\n",
      "71 0.921875\n",
      "Current learning_rate :  5.53725e-06\n",
      "Loss :  580.288\n",
      "72 0.87890625\n",
      "Current learning_rate :  5.47922e-06\n",
      "Loss :  581.076\n",
      "73 0.92578125\n",
      "Current learning_rate :  5.42179e-06\n",
      "Loss :  583.013\n",
      "74 0.90625\n",
      "Current learning_rate :  5.36497e-06\n",
      "Loss :  574.855\n",
      "75 0.94140625\n",
      "Current learning_rate :  5.30874e-06\n",
      "Loss :  569.896\n",
      "76 0.9375\n",
      "Current learning_rate :  5.2531e-06\n",
      "Loss :  600.491\n",
      "77 0.890625\n",
      "Current learning_rate :  5.19804e-06\n",
      "Loss :  603.754\n",
      "78 0.90625\n",
      "Current learning_rate :  5.14356e-06\n",
      "Loss :  575.146\n",
      "79 0.9453125\n",
      "Current learning_rate :  5.08965e-06\n",
      "Loss :  591.381\n",
      "80 0.91015625\n",
      "Current learning_rate :  5.03631e-06\n",
      "Loss :  595.034\n",
      "81 0.93359375\n",
      "Current learning_rate :  4.98353e-06\n",
      "Loss :  602.318\n",
      "82 0.91796875\n",
      "Current learning_rate :  4.9313e-06\n",
      "Loss :  591.145\n",
      "83 0.93359375\n",
      "Current learning_rate :  4.87961e-06\n",
      "Loss :  577.123\n",
      "84 0.8984375\n",
      "Current learning_rate :  4.82847e-06\n",
      "Loss :  583.404\n",
      "85 0.921875\n",
      "Current learning_rate :  4.77787e-06\n",
      "Loss :  622.026\n",
      "86 0.91015625\n",
      "Current learning_rate :  4.72779e-06\n",
      "Loss :  566.904\n",
      "87 0.91796875\n",
      "Current learning_rate :  4.67824e-06\n",
      "Loss :  621.334\n",
      "88 0.90234375\n",
      "Current learning_rate :  4.62921e-06\n",
      "Loss :  571.8\n",
      "89 0.93359375\n",
      "Current learning_rate :  4.58069e-06\n",
      "Loss :  584.836\n",
      "90 0.921875\n",
      "Current learning_rate :  4.53268e-06\n",
      "Loss :  577.403\n",
      "91 0.89453125\n",
      "Current learning_rate :  4.48517e-06\n",
      "Loss :  587.101\n",
      "92 0.92578125\n",
      "Current learning_rate :  4.43817e-06\n",
      "Loss :  585.315\n",
      "93 0.93359375\n",
      "Current learning_rate :  4.39165e-06\n",
      "Loss :  598.631\n",
      "94 0.953125\n",
      "Current learning_rate :  4.34562e-06\n",
      "Loss :  593.347\n",
      "95 0.9296875\n",
      "Current learning_rate :  4.30008e-06\n",
      "Loss :  587.688\n",
      "96 0.91015625\n",
      "Current learning_rate :  4.25501e-06\n",
      "Loss :  564.789\n",
      "97 0.91796875\n",
      "Current learning_rate :  4.21041e-06\n",
      "Loss :  587.016\n",
      "98 0.89453125\n",
      "Current learning_rate :  4.16629e-06\n",
      "Loss :  569.091\n",
      "99 0.91015625\n",
      "Current learning_rate :  4.12262e-06\n",
      "Loss :  605.062\n",
      "100 0.91015625\n",
      "Current learning_rate :  4.07941e-06\n",
      "Loss :  579.463\n",
      "101 0.92578125\n",
      "Current learning_rate :  4.03666e-06\n",
      "Loss :  573.492\n",
      "102 0.91796875\n",
      "Current learning_rate :  3.99435e-06\n",
      "Loss :  600.656\n",
      "103 0.9140625\n",
      "Current learning_rate :  3.95249e-06\n",
      "Loss :  623.261\n",
      "104 0.87109375\n",
      "Current learning_rate :  3.91106e-06\n",
      "Loss :  589.325\n",
      "105 0.91796875\n",
      "Current learning_rate :  3.87007e-06\n",
      "Loss :  620.235\n",
      "106 0.8828125\n",
      "Current learning_rate :  3.82951e-06\n",
      "Loss :  597.022\n",
      "107 0.9296875\n",
      "Current learning_rate :  3.78937e-06\n",
      "Loss :  612.537\n",
      "108 0.93359375\n",
      "Current learning_rate :  3.74966e-06\n",
      "Loss :  577.714\n",
      "109 0.89453125\n",
      "Current learning_rate :  3.71036e-06\n",
      "Loss :  579.363\n",
      "110 0.91796875\n",
      "Current learning_rate :  3.67147e-06\n",
      "Loss :  589.575\n",
      "111 0.890625\n",
      "Current learning_rate :  3.63299e-06\n",
      "Loss :  577.999\n",
      "112 0.92578125\n",
      "Current learning_rate :  3.59491e-06\n",
      "Loss :  566.57\n",
      "113 0.9375\n",
      "Current learning_rate :  3.55724e-06\n",
      "Loss :  610.332\n",
      "114 0.9296875\n",
      "Current learning_rate :  3.51995e-06\n",
      "Loss :  601.868\n",
      "115 0.90625\n",
      "Current learning_rate :  3.48306e-06\n",
      "Loss :  582.504\n",
      "116 0.9375\n",
      "Current learning_rate :  3.44656e-06\n",
      "Loss :  578.133\n",
      "117 0.9453125\n",
      "Current learning_rate :  3.41044e-06\n",
      "Loss :  599.605\n",
      "118 0.8828125\n",
      "Current learning_rate :  3.37469e-06\n",
      "Loss :  579.463\n",
      "119 0.8984375\n",
      "Current learning_rate :  3.33932e-06\n",
      "Loss :  580.977\n",
      "120 0.9375\n",
      "Current learning_rate :  3.30432e-06\n",
      "Loss :  603.335\n",
      "121 0.93359375\n",
      "Current learning_rate :  3.26969e-06\n",
      "Loss :  572.858\n",
      "122 0.91796875\n",
      "Current learning_rate :  3.23542e-06\n",
      "Loss :  579.061\n",
      "123 0.94921875\n",
      "Current learning_rate :  3.20151e-06\n",
      "Loss :  588.715\n",
      "124 0.90625\n",
      "Current learning_rate :  3.16796e-06\n",
      "Loss :  613.514\n",
      "125 0.91796875\n",
      "Current learning_rate :  3.13476e-06\n",
      "Loss :  553.707\n",
      "126 0.91796875\n",
      "Current learning_rate :  3.1019e-06\n",
      "Loss :  577.579\n",
      "127 0.9375\n",
      "Current learning_rate :  3.06939e-06\n",
      "Loss :  589.883\n",
      "128 0.921875\n",
      "Current learning_rate :  3.03722e-06\n",
      "Loss :  590.17\n",
      "129 0.91015625\n",
      "Current learning_rate :  3.00539e-06\n",
      "Loss :  588.387\n",
      "130 0.91015625\n",
      "Current learning_rate :  2.97389e-06\n",
      "Loss :  567.057\n",
      "131 0.9140625\n",
      "Current learning_rate :  2.94272e-06\n",
      "Loss :  581.484\n",
      "132 0.87890625\n",
      "Current learning_rate :  2.91188e-06\n",
      "Loss :  595.913\n",
      "133 0.91796875\n",
      "Current learning_rate :  2.88136e-06\n",
      "Loss :  591.379\n",
      "134 0.90625\n",
      "Current learning_rate :  2.85116e-06\n",
      "Loss :  618.383\n",
      "135 0.90625\n",
      "Current learning_rate :  2.82128e-06\n",
      "Loss :  610.185\n",
      "136 0.90234375\n",
      "Current learning_rate :  2.79171e-06\n",
      "Loss :  625.053\n",
      "137 0.9140625\n",
      "Current learning_rate :  2.76245e-06\n",
      "Loss :  591.848\n",
      "138 0.921875\n",
      "Current learning_rate :  2.7335e-06\n",
      "Loss :  576.332\n",
      "139 0.9375\n",
      "Current learning_rate :  2.70485e-06\n",
      "Loss :  575.35\n",
      "140 0.9140625\n",
      "Current learning_rate :  2.6765e-06\n",
      "Loss :  590.13\n",
      "141 0.88671875\n",
      "Current learning_rate :  2.64845e-06\n",
      "Loss :  585.896\n",
      "142 0.9296875\n",
      "Current learning_rate :  2.62069e-06\n",
      "Loss :  592.26\n",
      "143 0.90625\n",
      "Current learning_rate :  2.59323e-06\n",
      "Loss :  581.039\n",
      "144 0.90625\n",
      "Current learning_rate :  2.56605e-06\n",
      "Loss :  580.948\n",
      "145 0.921875\n",
      "Current learning_rate :  2.53915e-06\n",
      "Loss :  575.127\n",
      "146 0.921875\n",
      "Current learning_rate :  2.51254e-06\n",
      "Loss :  610.432\n",
      "147 0.92578125\n",
      "Current learning_rate :  2.48621e-06\n",
      "Loss :  596.402\n",
      "148 0.90625\n",
      "Current learning_rate :  2.46015e-06\n",
      "Loss :  600.809\n",
      "149 0.90625\n",
      "Current learning_rate :  2.43437e-06\n",
      "Loss :  579.638\n",
      "150 0.91015625\n",
      "Current learning_rate :  2.40885e-06\n",
      "Loss :  566.71\n",
      "151 0.93359375\n",
      "Current learning_rate :  2.38361e-06\n",
      "Loss :  594.511\n",
      "152 0.875\n",
      "Current learning_rate :  2.35862e-06\n",
      "Loss :  591.849\n",
      "153 0.9140625\n",
      "Current learning_rate :  2.3339e-06\n",
      "Loss :  587.537\n",
      "154 0.9296875\n",
      "Current learning_rate :  2.30944e-06\n",
      "Loss :  582.439\n",
      "155 0.921875\n",
      "Current learning_rate :  2.28524e-06\n",
      "Loss :  574.728\n",
      "156 0.94921875\n",
      "Current learning_rate :  2.26129e-06\n",
      "Loss :  584.358\n",
      "157 0.9375\n",
      "Current learning_rate :  2.23759e-06\n",
      "Loss :  592.792\n",
      "158 0.9375\n",
      "Current learning_rate :  2.21414e-06\n",
      "Loss :  588.816\n",
      "159 0.92578125\n",
      "Current learning_rate :  2.19093e-06\n",
      "Loss :  592.056\n",
      "160 0.92578125\n",
      "Current learning_rate :  2.16797e-06\n",
      "Loss :  601.076\n",
      "161 0.91015625\n",
      "Current learning_rate :  2.14524e-06\n",
      "Loss :  581.622\n",
      "162 0.9375\n",
      "Current learning_rate :  2.12276e-06\n",
      "Loss :  584.078\n",
      "163 0.87890625\n",
      "Current learning_rate :  2.10051e-06\n",
      "Loss :  571.821\n",
      "164 0.9140625\n",
      "Current learning_rate :  2.0785e-06\n",
      "Loss :  582.438\n",
      "165 0.9453125\n",
      "Current learning_rate :  2.05671e-06\n",
      "Loss :  574.748\n",
      "166 0.92578125\n",
      "Current learning_rate :  2.03516e-06\n",
      "Loss :  560.495\n",
      "167 0.90234375\n",
      "Current learning_rate :  2.01383e-06\n",
      "Loss :  604.569\n",
      "168 0.90625\n",
      "Current learning_rate :  1.99272e-06\n",
      "Loss :  581.858\n",
      "169 0.92578125\n",
      "Current learning_rate :  1.97184e-06\n",
      "Loss :  618.36\n",
      "170 0.91015625\n",
      "Current learning_rate :  1.95117e-06\n",
      "Loss :  606.096\n",
      "171 0.890625\n",
      "Current learning_rate :  1.93072e-06\n",
      "Loss :  587.492\n",
      "172 0.90625\n",
      "Current learning_rate :  1.91049e-06\n",
      "Loss :  593.955\n",
      "173 0.9375\n",
      "Current learning_rate :  1.89046e-06\n",
      "Loss :  567.487\n",
      "174 0.94140625\n",
      "Current learning_rate :  1.87065e-06\n",
      "Loss :  579.913\n",
      "175 0.9375\n",
      "Current learning_rate :  1.85104e-06\n",
      "Loss :  585.751\n",
      "176 0.8984375\n",
      "Current learning_rate :  1.83164e-06\n",
      "Loss :  579.887\n",
      "177 0.92578125\n",
      "Current learning_rate :  1.81245e-06\n",
      "Loss :  592.921\n",
      "178 0.953125\n",
      "Current learning_rate :  1.79345e-06\n",
      "Loss :  573.476\n",
      "179 0.9296875\n",
      "Current learning_rate :  1.77465e-06\n",
      "Loss :  585.206\n",
      "180 0.90625\n",
      "Current learning_rate :  1.75605e-06\n",
      "Loss :  615.126\n",
      "181 0.90625\n",
      "Current learning_rate :  1.73765e-06\n",
      "Loss :  600.36\n",
      "182 0.9140625\n",
      "Current learning_rate :  1.71944e-06\n",
      "Loss :  609.714\n",
      "183 0.9140625\n",
      "Current learning_rate :  1.70142e-06\n",
      "Loss :  574.542\n",
      "184 0.90234375\n",
      "Current learning_rate :  1.68358e-06\n",
      "Loss :  606.737\n",
      "185 0.9375\n",
      "Current learning_rate :  1.66594e-06\n",
      "Loss :  595.352\n",
      "186 0.9296875\n",
      "Current learning_rate :  1.64848e-06\n",
      "Loss :  581.404\n",
      "187 0.95703125\n",
      "Current learning_rate :  1.6312e-06\n",
      "Loss :  589.589\n",
      "188 0.89453125\n",
      "Current learning_rate :  1.6141e-06\n",
      "Loss :  603.866\n",
      "189 0.91796875\n",
      "Current learning_rate :  1.59719e-06\n",
      "Loss :  589.703\n",
      "190 0.94140625\n",
      "Current learning_rate :  1.58045e-06\n",
      "Loss :  595.042\n",
      "191 0.90234375\n",
      "Current learning_rate :  1.56388e-06\n",
      "Loss :  581.72\n",
      "192 0.93359375\n",
      "Current learning_rate :  1.54749e-06\n",
      "Loss :  572.615\n",
      "193 0.93359375\n",
      "Current learning_rate :  1.53127e-06\n",
      "Loss :  578.729\n",
      "194 0.91015625\n",
      "Current learning_rate :  1.51522e-06\n",
      "Loss :  579.907\n",
      "195 0.8984375\n",
      "Current learning_rate :  1.49934e-06\n",
      "Loss :  579.579\n",
      "196 0.921875\n",
      "Current learning_rate :  1.48363e-06\n",
      "Loss :  621.008\n",
      "197 0.94140625\n",
      "Current learning_rate :  1.46808e-06\n",
      "Loss :  577.774\n",
      "198 0.90625\n",
      "Current learning_rate :  1.45269e-06\n",
      "Loss :  577.28\n",
      "199 0.9296875\n",
      "Current learning_rate :  1.43747e-06\n",
      "Loss :  570.366\n",
      "200 0.9296875\n",
      "Current learning_rate :  1.4224e-06\n",
      "Loss :  583.418\n",
      "201 0.9296875\n",
      "Current learning_rate :  1.40749e-06\n",
      "Loss :  607.836\n",
      "202 0.87890625\n",
      "Current learning_rate :  1.39274e-06\n",
      "Loss :  581.259\n",
      "203 0.8828125\n",
      "Current learning_rate :  1.37815e-06\n",
      "Loss :  587.618\n",
      "204 0.9375\n",
      "Current learning_rate :  1.3637e-06\n",
      "Loss :  579.036\n",
      "205 0.91015625\n",
      "Current learning_rate :  1.34941e-06\n",
      "Loss :  582.513\n",
      "206 0.9296875\n",
      "Current learning_rate :  1.33527e-06\n",
      "Loss :  568.323\n",
      "207 0.92578125\n",
      "Current learning_rate :  1.32127e-06\n",
      "Loss :  601.881\n",
      "208 0.90625\n",
      "Current learning_rate :  1.30742e-06\n",
      "Loss :  606.365\n",
      "209 0.91796875\n",
      "Current learning_rate :  1.29372e-06\n",
      "Loss :  597.828\n",
      "210 0.9375\n",
      "Current learning_rate :  1.28016e-06\n",
      "Loss :  578.562\n",
      "211 0.92578125\n",
      "Current learning_rate :  1.26675e-06\n",
      "Loss :  604.734\n",
      "212 0.91796875\n",
      "Current learning_rate :  1.25347e-06\n",
      "Loss :  579.103\n",
      "213 0.91015625\n",
      "Current learning_rate :  1.24033e-06\n",
      "Loss :  582.899\n",
      "214 0.8984375\n",
      "Current learning_rate :  1.22733e-06\n",
      "Loss :  605.715\n",
      "215 0.9140625\n",
      "Current learning_rate :  1.21447e-06\n",
      "Loss :  576.475\n",
      "216 0.90625\n",
      "Current learning_rate :  1.20174e-06\n",
      "Loss :  602.339\n",
      "217 0.89453125\n",
      "Current learning_rate :  1.18915e-06\n",
      "Loss :  579.622\n",
      "218 0.890625\n",
      "Current learning_rate :  1.17668e-06\n",
      "Loss :  594.041\n",
      "219 0.90234375\n",
      "Current learning_rate :  1.16435e-06\n",
      "Loss :  603.398\n",
      "220 0.94921875\n",
      "Current learning_rate :  1.15215e-06\n",
      "Loss :  598.184\n",
      "221 0.921875\n",
      "Current learning_rate :  1.14007e-06\n",
      "Loss :  586.989\n",
      "222 0.91796875\n",
      "Current learning_rate :  1.12812e-06\n",
      "Loss :  609.492\n",
      "223 0.90234375\n",
      "Current learning_rate :  1.1163e-06\n",
      "Loss :  602.846\n",
      "224 0.92578125\n",
      "Current learning_rate :  1.1046e-06\n",
      "Loss :  585.325\n",
      "225 0.921875\n",
      "Current learning_rate :  1.09302e-06\n",
      "Loss :  603.784\n",
      "226 0.9375\n",
      "Current learning_rate :  1.08157e-06\n",
      "Loss :  576.49\n",
      "227 0.8984375\n",
      "Current learning_rate :  1.07023e-06\n",
      "Loss :  582.678\n",
      "228 0.9375\n",
      "Current learning_rate :  1.05901e-06\n",
      "Loss :  591.207\n",
      "229 0.90625\n",
      "Current learning_rate :  1.04791e-06\n",
      "Loss :  593.679\n",
      "230 0.90625\n",
      "Current learning_rate :  1.03693e-06\n",
      "Loss :  593.895\n",
      "231 0.90625\n",
      "Current learning_rate :  1.02606e-06\n",
      "Loss :  604.13\n",
      "232 0.9296875\n",
      "Current learning_rate :  1.01531e-06\n",
      "Loss :  579.88\n",
      "233 0.8984375\n",
      "Current learning_rate :  1.00467e-06\n",
      "Loss :  592.765\n",
      "234 0.91796875\n",
      "Current learning_rate :  9.94139e-07\n",
      "Loss :  574.051\n",
      "235 0.94140625\n",
      "Current learning_rate :  9.8372e-07\n",
      "Loss :  591.207\n",
      "236 0.92578125\n",
      "Current learning_rate :  9.73409e-07\n",
      "Loss :  579.638\n",
      "237 0.9296875\n",
      "Current learning_rate :  9.63208e-07\n",
      "Loss :  573.711\n",
      "238 0.921875\n",
      "Current learning_rate :  9.53112e-07\n",
      "Loss :  577.486\n",
      "239 0.90234375\n",
      "Current learning_rate :  9.43123e-07\n",
      "Loss :  555.512\n",
      "240 0.8984375\n",
      "Current learning_rate :  9.33238e-07\n",
      "Loss :  597.832\n",
      "241 0.921875\n",
      "Current learning_rate :  9.23457e-07\n",
      "Loss :  611.96\n",
      "242 0.9296875\n",
      "Current learning_rate :  9.13779e-07\n",
      "Loss :  598.553\n",
      "243 0.92578125\n",
      "Current learning_rate :  9.04202e-07\n",
      "Loss :  615.871\n",
      "244 0.890625\n",
      "Current learning_rate :  8.94725e-07\n",
      "Loss :  575.568\n",
      "245 0.9140625\n",
      "Current learning_rate :  8.85348e-07\n",
      "Loss :  604.651\n",
      "246 0.92578125\n",
      "Current learning_rate :  8.76068e-07\n",
      "Loss :  576.738\n",
      "247 0.89453125\n",
      "Current learning_rate :  8.66887e-07\n",
      "Loss :  576.758\n",
      "248 0.8984375\n",
      "Current learning_rate :  8.57801e-07\n",
      "Loss :  600.007\n",
      "249 0.91015625\n",
      "Current learning_rate :  8.48811e-07\n",
      "Loss :  583.377\n",
      "250 0.921875\n",
      "Current learning_rate :  8.39915e-07\n",
      "Loss :  563.473\n",
      "251 0.91015625\n",
      "Current learning_rate :  8.31111e-07\n",
      "Loss :  586.331\n",
      "252 0.9375\n",
      "Current learning_rate :  8.22401e-07\n",
      "Loss :  599.343\n",
      "253 0.9296875\n",
      "Current learning_rate :  8.13782e-07\n",
      "Loss :  577.534\n",
      "254 0.921875\n",
      "Current learning_rate :  8.05252e-07\n",
      "Loss :  592.916\n",
      "255 0.9375\n",
      "Current learning_rate :  7.96813e-07\n",
      "Loss :  594.655\n",
      "256 0.91796875\n",
      "Current learning_rate :  7.88462e-07\n",
      "Loss :  592.814\n",
      "257 0.94921875\n",
      "Current learning_rate :  7.80198e-07\n",
      "Loss :  609.748\n",
      "258 0.9296875\n",
      "Current learning_rate :  7.72021e-07\n",
      "Loss :  586.328\n",
      "259 0.91796875\n",
      "Current learning_rate :  7.6393e-07\n",
      "Loss :  590.2\n",
      "260 0.9140625\n",
      "Current learning_rate :  7.55923e-07\n",
      "Loss :  575.515\n",
      "261 0.90234375\n",
      "Current learning_rate :  7.48e-07\n",
      "Loss :  613.799\n",
      "262 0.921875\n",
      "Current learning_rate :  7.40161e-07\n",
      "Loss :  566.828\n",
      "263 0.921875\n",
      "Current learning_rate :  7.32404e-07\n",
      "Loss :  579.619\n",
      "264 0.8984375\n",
      "Current learning_rate :  7.24727e-07\n",
      "Loss :  587.774\n",
      "265 0.921875\n",
      "Current learning_rate :  7.17132e-07\n",
      "Loss :  589.365\n",
      "266 0.88671875\n",
      "Current learning_rate :  7.09615e-07\n",
      "Loss :  604.958\n",
      "267 0.92578125\n",
      "Current learning_rate :  7.02178e-07\n",
      "Loss :  574.525\n",
      "268 0.91015625\n",
      "Current learning_rate :  6.94819e-07\n",
      "Loss :  574.103\n",
      "269 0.90625\n",
      "Current learning_rate :  6.87537e-07\n",
      "Loss :  609.363\n",
      "270 0.92578125\n",
      "Current learning_rate :  6.80331e-07\n",
      "Loss :  587.496\n",
      "271 0.94140625\n",
      "Current learning_rate :  6.732e-07\n",
      "Loss :  577.036\n",
      "272 0.91796875\n",
      "Current learning_rate :  6.66145e-07\n",
      "Loss :  584.424\n",
      "273 0.90625\n",
      "Current learning_rate :  6.59163e-07\n",
      "Loss :  604.182\n",
      "274 0.890625\n",
      "Current learning_rate :  6.52254e-07\n",
      "Loss :  567.202\n",
      "275 0.9375\n",
      "Current learning_rate :  6.45418e-07\n",
      "Loss :  591.877\n",
      "276 0.9296875\n",
      "Current learning_rate :  6.38654e-07\n",
      "Loss :  581.112\n",
      "277 0.9140625\n",
      "Current learning_rate :  6.3196e-07\n",
      "Loss :  562.08\n",
      "278 0.92578125\n",
      "Current learning_rate :  6.25337e-07\n",
      "Loss :  588.611\n",
      "279 0.8828125\n",
      "Current learning_rate :  6.18783e-07\n",
      "Loss :  601.561\n",
      "280 0.9140625\n",
      "Current learning_rate :  6.12298e-07\n",
      "Loss :  567.389\n",
      "281 0.93359375\n",
      "Current learning_rate :  6.0588e-07\n",
      "Loss :  611.679\n",
      "282 0.91015625\n",
      "Current learning_rate :  5.9953e-07\n",
      "Loss :  578.338\n",
      "283 0.921875\n",
      "Current learning_rate :  5.93247e-07\n",
      "Loss :  583.512\n",
      "284 0.93359375\n",
      "Current learning_rate :  5.87029e-07\n",
      "Loss :  572.852\n",
      "285 0.890625\n",
      "Current learning_rate :  5.80877e-07\n",
      "Loss :  576.892\n",
      "286 0.93359375\n",
      "Current learning_rate :  5.74789e-07\n",
      "Loss :  576.847\n",
      "287 0.9375\n",
      "Current learning_rate :  5.68765e-07\n",
      "Loss :  563.614\n",
      "288 0.9140625\n",
      "Current learning_rate :  5.62803e-07\n",
      "Loss :  586.757\n",
      "289 0.90234375\n",
      "Current learning_rate :  5.56905e-07\n",
      "Loss :  584.874\n",
      "290 0.91796875\n",
      "Current learning_rate :  5.51068e-07\n",
      "Loss :  588.442\n",
      "291 0.93359375\n",
      "Current learning_rate :  5.45292e-07\n",
      "Loss :  589.586\n",
      "292 0.90625\n",
      "Current learning_rate :  5.39577e-07\n",
      "Loss :  579.956\n",
      "293 0.91015625\n",
      "Current learning_rate :  5.33922e-07\n",
      "Loss :  585.689\n",
      "294 0.91796875\n",
      "Current learning_rate :  5.28326e-07\n",
      "Loss :  578.132\n",
      "295 0.921875\n",
      "Current learning_rate :  5.22789e-07\n",
      "Loss :  598.585\n",
      "296 0.9296875\n",
      "Current learning_rate :  5.1731e-07\n",
      "Loss :  586.663\n",
      "297 0.890625\n",
      "Current learning_rate :  5.11888e-07\n",
      "Loss :  612.354\n",
      "298 0.921875\n",
      "Current learning_rate :  5.06523e-07\n",
      "Loss :  570.559\n",
      "299 0.89453125\n",
      "Current learning_rate :  5.01214e-07\n",
      "Loss :  591.929\n",
      "300 0.91015625\n",
      "Current learning_rate :  4.95961e-07\n",
      "Loss :  570.686\n",
      "301 0.890625\n",
      "Current learning_rate :  4.90763e-07\n",
      "Loss :  581.289\n",
      "302 0.90234375\n",
      "Current learning_rate :  4.8562e-07\n",
      "Loss :  599.409\n",
      "303 0.90625\n",
      "Current learning_rate :  4.8053e-07\n",
      "Loss :  569.416\n",
      "304 0.91015625\n",
      "Current learning_rate :  4.75493e-07\n",
      "Loss :  599.477\n",
      "305 0.90234375\n",
      "Current learning_rate :  4.7051e-07\n",
      "Loss :  577.684\n",
      "306 0.90625\n",
      "Current learning_rate :  4.65579e-07\n",
      "Loss :  569.15\n",
      "307 0.8828125\n",
      "Current learning_rate :  4.60699e-07\n",
      "Loss :  590.877\n",
      "308 0.8984375\n",
      "Current learning_rate :  4.5587e-07\n",
      "Loss :  564.295\n",
      "309 0.9296875\n",
      "Current learning_rate :  4.51093e-07\n",
      "Loss :  579.408\n",
      "310 0.91796875\n",
      "Current learning_rate :  4.46365e-07\n",
      "Loss :  595.969\n",
      "311 0.93359375\n",
      "Current learning_rate :  4.41687e-07\n",
      "Loss :  593.283\n",
      "312 0.91796875\n",
      "Current learning_rate :  4.37058e-07\n",
      "Loss :  576.11\n",
      "313 0.91796875\n",
      "Current learning_rate :  4.32477e-07\n",
      "Loss :  616.809\n",
      "314 0.8984375\n",
      "Current learning_rate :  4.27944e-07\n",
      "Loss :  586.038\n",
      "315 0.890625\n",
      "Current learning_rate :  4.23459e-07\n",
      "Loss :  576.814\n",
      "316 0.91796875\n",
      "Current learning_rate :  4.19021e-07\n",
      "Loss :  597.294\n",
      "317 0.9453125\n",
      "Current learning_rate :  4.14629e-07\n",
      "Loss :  594.564\n",
      "318 0.91015625\n",
      "Current learning_rate :  4.10283e-07\n",
      "Loss :  602.131\n",
      "319 0.90625\n",
      "Current learning_rate :  4.05983e-07\n",
      "Loss :  578.675\n",
      "320 0.9296875\n",
      "Current learning_rate :  4.01728e-07\n",
      "Loss :  621.917\n",
      "321 0.90625\n",
      "Current learning_rate :  3.97518e-07\n",
      "Loss :  579.625\n",
      "322 0.8984375\n",
      "Current learning_rate :  3.93352e-07\n",
      "Loss :  585.844\n",
      "323 0.9296875\n",
      "Current learning_rate :  3.89229e-07\n",
      "Loss :  583.461\n",
      "324 0.94921875\n",
      "Current learning_rate :  3.8515e-07\n",
      "Loss :  583.616\n",
      "325 0.94140625\n",
      "Current learning_rate :  3.81113e-07\n",
      "Loss :  569.163\n",
      "326 0.92578125\n",
      "Current learning_rate :  3.77119e-07\n",
      "Loss :  605.931\n",
      "327 0.9296875\n",
      "Current learning_rate :  3.73166e-07\n",
      "Loss :  591.973\n",
      "328 0.921875\n",
      "Current learning_rate :  3.69255e-07\n",
      "Loss :  590.122\n",
      "329 0.91015625\n",
      "Current learning_rate :  3.65385e-07\n",
      "Loss :  575.938\n",
      "330 0.9296875\n",
      "Current learning_rate :  3.61556e-07\n",
      "Loss :  580.542\n",
      "331 0.9140625\n",
      "Current learning_rate :  3.57766e-07\n",
      "Loss :  584.565\n",
      "332 0.93359375\n",
      "Current learning_rate :  3.54017e-07\n",
      "Loss :  589.41\n",
      "333 0.86328125\n",
      "Current learning_rate :  3.50306e-07\n",
      "Loss :  589.015\n",
      "334 0.91796875\n",
      "Current learning_rate :  3.46635e-07\n",
      "Loss :  590.179\n",
      "335 0.9296875\n",
      "Current learning_rate :  3.43002e-07\n",
      "Loss :  609.279\n",
      "336 0.87890625\n",
      "Current learning_rate :  3.39407e-07\n",
      "Loss :  580.248\n",
      "337 0.91015625\n",
      "Current learning_rate :  3.3585e-07\n",
      "Loss :  602.746\n",
      "338 0.92578125\n",
      "Current learning_rate :  3.3233e-07\n",
      "Loss :  595.091\n",
      "339 0.90625\n",
      "Current learning_rate :  3.28847e-07\n",
      "Loss :  581.373\n",
      "340 0.88671875\n",
      "Current learning_rate :  3.254e-07\n",
      "Loss :  595.124\n",
      "341 0.90234375\n",
      "Current learning_rate :  3.2199e-07\n",
      "Loss :  585.461\n",
      "342 0.92578125\n",
      "Current learning_rate :  3.18615e-07\n",
      "Loss :  621.079\n",
      "343 0.92578125\n",
      "Current learning_rate :  3.15275e-07\n",
      "Loss :  591.757\n",
      "344 0.90234375\n",
      "Current learning_rate :  3.11971e-07\n",
      "Loss :  611.527\n",
      "345 0.9296875\n",
      "Current learning_rate :  3.08702e-07\n",
      "Loss :  625.417\n",
      "346 0.92578125\n",
      "Current learning_rate :  3.05466e-07\n",
      "Loss :  581.093\n",
      "347 0.9296875\n",
      "Current learning_rate :  3.02265e-07\n",
      "Loss :  582.816\n",
      "348 0.93359375\n",
      "Current learning_rate :  2.99097e-07\n",
      "Loss :  584.664\n",
      "349 0.90625\n",
      "Current learning_rate :  2.95962e-07\n",
      "Loss :  602.07\n",
      "350 0.92578125\n",
      "Current learning_rate :  2.9286e-07\n",
      "Loss :  595.231\n",
      "351 0.94140625\n",
      "Current learning_rate :  2.89791e-07\n",
      "Loss :  615.444\n",
      "352 0.92578125\n",
      "Current learning_rate :  2.86754e-07\n",
      "Loss :  586.427\n",
      "353 0.9375\n",
      "Current learning_rate :  2.83748e-07\n",
      "Loss :  607.222\n",
      "354 0.92578125\n",
      "Current learning_rate :  2.80774e-07\n",
      "Loss :  600.453\n",
      "355 0.92578125\n",
      "Current learning_rate :  2.77831e-07\n",
      "Loss :  583.547\n",
      "356 0.90625\n",
      "Current learning_rate :  2.7492e-07\n",
      "Loss :  604.087\n",
      "357 0.91015625\n",
      "Current learning_rate :  2.72038e-07\n",
      "Loss :  604.441\n",
      "358 0.92578125\n",
      "Current learning_rate :  2.69187e-07\n",
      "Loss :  586.249\n",
      "359 0.9609375\n",
      "Current learning_rate :  2.66366e-07\n",
      "Loss :  585.375\n",
      "360 0.90234375\n",
      "Current learning_rate :  2.63574e-07\n",
      "Loss :  589.756\n",
      "361 0.8671875\n",
      "Current learning_rate :  2.60812e-07\n",
      "Loss :  584.93\n",
      "362 0.94921875\n",
      "Current learning_rate :  2.58078e-07\n",
      "Loss :  598.233\n",
      "363 0.9375\n",
      "Current learning_rate :  2.55373e-07\n",
      "Loss :  575.839\n",
      "364 0.90625\n",
      "Current learning_rate :  2.52697e-07\n",
      "Loss :  569.69\n",
      "365 0.92578125\n",
      "Current learning_rate :  2.50048e-07\n",
      "Loss :  587.075\n",
      "366 0.8984375\n",
      "Current learning_rate :  2.47428e-07\n",
      "Loss :  581.468\n",
      "367 0.9296875\n",
      "Current learning_rate :  2.44834e-07\n",
      "Loss :  604.348\n",
      "368 0.91015625\n",
      "Current learning_rate :  2.42268e-07\n",
      "Loss :  594.159\n",
      "369 0.9140625\n",
      "Current learning_rate :  2.39729e-07\n",
      "Loss :  574.366\n",
      "370 0.859375\n",
      "Current learning_rate :  2.37217e-07\n",
      "Loss :  593.998\n",
      "371 0.92578125\n",
      "Current learning_rate :  2.3473e-07\n",
      "Loss :  589.507\n",
      "372 0.9296875\n",
      "Current learning_rate :  2.3227e-07\n",
      "Loss :  573.291\n",
      "373 0.91015625\n",
      "Current learning_rate :  2.29836e-07\n",
      "Loss :  578.52\n",
      "374 0.93359375\n",
      "Current learning_rate :  2.27427e-07\n",
      "Loss :  590.741\n",
      "375 0.9140625\n",
      "Current learning_rate :  2.25043e-07\n",
      "Loss :  602.583\n",
      "376 0.9140625\n",
      "Current learning_rate :  2.22685e-07\n",
      "Loss :  582.482\n",
      "377 0.89453125\n",
      "Current learning_rate :  2.20351e-07\n",
      "Loss :  605.102\n",
      "378 0.91796875\n",
      "Current learning_rate :  2.18041e-07\n",
      "Loss :  598.867\n",
      "379 0.9296875\n",
      "Current learning_rate :  2.15756e-07\n",
      "Loss :  587.549\n",
      "380 0.92578125\n",
      "Current learning_rate :  2.13495e-07\n",
      "Loss :  599.751\n",
      "381 0.91796875\n",
      "Current learning_rate :  2.11257e-07\n",
      "Loss :  599.939\n",
      "382 0.921875\n",
      "Current learning_rate :  2.09043e-07\n",
      "Loss :  566.067\n",
      "383 0.92578125\n",
      "Current learning_rate :  2.06852e-07\n",
      "Loss :  594.178\n",
      "384 0.92578125\n",
      "Current learning_rate :  2.04684e-07\n",
      "Loss :  592.622\n",
      "385 0.9375\n",
      "Current learning_rate :  2.02539e-07\n",
      "Loss :  586.312\n",
      "386 0.90625\n",
      "Current learning_rate :  2.00416e-07\n",
      "Loss :  574.92\n",
      "387 0.91015625\n",
      "Current learning_rate :  1.98316e-07\n",
      "Loss :  588.887\n",
      "388 0.890625\n",
      "Current learning_rate :  1.96237e-07\n",
      "Loss :  594.189\n",
      "389 0.94140625\n",
      "Current learning_rate :  1.94181e-07\n",
      "Loss :  589.206\n",
      "390 0.921875\n",
      "Current learning_rate :  1.92145e-07\n",
      "Loss :  561.248\n",
      "391 0.8671875\n",
      "Current learning_rate :  1.90132e-07\n",
      "Loss :  591.943\n",
      "392 0.9453125\n",
      "Current learning_rate :  1.88139e-07\n",
      "Loss :  586.504\n",
      "393 0.921875\n",
      "Current learning_rate :  1.86167e-07\n",
      "Loss :  600.022\n",
      "394 0.9140625\n",
      "Current learning_rate :  1.84216e-07\n",
      "Loss :  603.143\n",
      "395 0.890625\n",
      "Current learning_rate :  1.82285e-07\n",
      "Loss :  575.933\n",
      "396 0.8984375\n",
      "Current learning_rate :  1.80375e-07\n",
      "Loss :  588.081\n",
      "397 0.92578125\n",
      "Current learning_rate :  1.78484e-07\n",
      "Loss :  571.864\n",
      "398 0.9375\n",
      "Current learning_rate :  1.76614e-07\n",
      "Loss :  588.256\n",
      "399 0.91015625\n",
      "Current learning_rate :  1.74763e-07\n",
      "Loss :  579.617\n",
      "400 0.91015625\n",
      "Current learning_rate :  1.72931e-07\n",
      "Loss :  575.235\n",
      "401 0.93359375\n",
      "Current learning_rate :  1.71118e-07\n",
      "Loss :  575.701\n",
      "402 0.9140625\n",
      "Current learning_rate :  1.69325e-07\n",
      "Loss :  578.385\n",
      "403 0.921875\n",
      "Current learning_rate :  1.6755e-07\n",
      "Loss :  608.27\n",
      "404 0.90625\n",
      "Current learning_rate :  1.65794e-07\n",
      "Loss :  584.533\n",
      "405 0.921875\n",
      "Current learning_rate :  1.64057e-07\n",
      "Loss :  600.169\n",
      "406 0.9140625\n",
      "Current learning_rate :  1.62337e-07\n",
      "Loss :  613.515\n",
      "407 0.9140625\n",
      "Current learning_rate :  1.60636e-07\n",
      "Loss :  591.773\n",
      "408 0.91796875\n",
      "Current learning_rate :  1.58952e-07\n",
      "Loss :  600.707\n",
      "409 0.90625\n",
      "Current learning_rate :  1.57286e-07\n",
      "Loss :  628.429\n",
      "410 0.91015625\n",
      "Current learning_rate :  1.55638e-07\n",
      "Loss :  614.623\n",
      "411 0.9140625\n",
      "Current learning_rate :  1.54007e-07\n",
      "Loss :  578.845\n",
      "412 0.9375\n",
      "Current learning_rate :  1.52393e-07\n",
      "Loss :  568.328\n",
      "413 0.8984375\n",
      "Current learning_rate :  1.50795e-07\n",
      "Loss :  594.832\n",
      "414 0.9375\n",
      "Current learning_rate :  1.49215e-07\n",
      "Loss :  604.051\n",
      "415 0.94140625\n",
      "Current learning_rate :  1.47651e-07\n",
      "Loss :  598.872\n",
      "416 0.9140625\n",
      "Current learning_rate :  1.46104e-07\n",
      "Loss :  629.468\n",
      "417 0.92578125\n",
      "Current learning_rate :  1.44572e-07\n",
      "Loss :  591.886\n",
      "418 0.9375\n",
      "Current learning_rate :  1.43057e-07\n",
      "Loss :  594.84\n",
      "419 0.92578125\n",
      "Current learning_rate :  1.41558e-07\n",
      "Loss :  579.875\n",
      "420 0.921875\n",
      "Current learning_rate :  1.40074e-07\n",
      "Loss :  554.417\n",
      "421 0.8984375\n",
      "Current learning_rate :  1.38606e-07\n",
      "Loss :  598.707\n",
      "422 0.9140625\n",
      "Current learning_rate :  1.37153e-07\n",
      "Loss :  595.415\n",
      "423 0.89453125\n",
      "Current learning_rate :  1.35716e-07\n",
      "Loss :  612.338\n",
      "424 0.90234375\n",
      "Current learning_rate :  1.34293e-07\n",
      "Loss :  576.457\n",
      "425 0.87890625\n",
      "Current learning_rate :  1.32886e-07\n",
      "Loss :  594.898\n",
      "426 0.90625\n",
      "Current learning_rate :  1.31493e-07\n",
      "Loss :  584.598\n",
      "427 0.9296875\n",
      "Current learning_rate :  1.30115e-07\n",
      "Loss :  579.28\n",
      "428 0.9296875\n",
      "Current learning_rate :  1.28751e-07\n",
      "Loss :  572.89\n",
      "429 0.953125\n",
      "Current learning_rate :  1.27402e-07\n",
      "Loss :  584.211\n",
      "430 0.91796875\n",
      "Current learning_rate :  1.26067e-07\n",
      "Loss :  580.369\n",
      "431 0.921875\n",
      "Current learning_rate :  1.24745e-07\n",
      "Loss :  586.265\n",
      "432 0.9375\n",
      "Current learning_rate :  1.23438e-07\n",
      "Loss :  579.283\n",
      "433 0.88671875\n",
      "Current learning_rate :  1.22144e-07\n",
      "Loss :  558.663\n",
      "434 0.890625\n",
      "Current learning_rate :  1.20864e-07\n",
      "Loss :  595.717\n",
      "435 0.921875\n",
      "Current learning_rate :  1.19597e-07\n",
      "Loss :  589.376\n",
      "436 0.89453125\n",
      "Current learning_rate :  1.18344e-07\n",
      "Loss :  579.207\n",
      "437 0.90625\n",
      "Current learning_rate :  1.17104e-07\n",
      "Loss :  586.286\n",
      "438 0.91015625\n",
      "Current learning_rate :  1.15876e-07\n",
      "Loss :  585.505\n",
      "439 0.90625\n",
      "Current learning_rate :  1.14662e-07\n",
      "Loss :  595.046\n",
      "440 0.9375\n",
      "Current learning_rate :  1.1346e-07\n",
      "Loss :  570.95\n",
      "441 0.90234375\n",
      "Current learning_rate :  1.12271e-07\n",
      "Loss :  582.778\n",
      "442 0.890625\n",
      "Current learning_rate :  1.11094e-07\n",
      "Loss :  611.929\n",
      "443 0.953125\n",
      "Current learning_rate :  1.0993e-07\n",
      "Loss :  582.376\n",
      "444 0.9140625\n",
      "Current learning_rate :  1.08778e-07\n",
      "Loss :  578.736\n",
      "445 0.91015625\n",
      "Current learning_rate :  1.07638e-07\n",
      "Loss :  570.292\n",
      "446 0.8984375\n",
      "Current learning_rate :  1.06509e-07\n",
      "Loss :  598.255\n",
      "447 0.92578125\n",
      "Current learning_rate :  1.05393e-07\n",
      "Loss :  574.565\n",
      "448 0.91015625\n",
      "Current learning_rate :  1.04289e-07\n",
      "Loss :  568.94\n",
      "449 0.9140625\n",
      "Current learning_rate :  1.03196e-07\n",
      "Loss :  601.317\n",
      "450 0.921875\n",
      "Current learning_rate :  1.02114e-07\n",
      "Loss :  595.24\n",
      "451 0.91796875\n",
      "Current learning_rate :  1.01044e-07\n",
      "Loss :  599.99\n",
      "452 0.9453125\n",
      "Current learning_rate :  9.99847e-08\n",
      "Loss :  582.726\n",
      "453 0.921875\n",
      "Current learning_rate :  9.89368e-08\n",
      "Loss :  605.276\n",
      "454 0.93359375\n",
      "Current learning_rate :  9.78999e-08\n",
      "Loss :  567.252\n",
      "455 0.91015625\n",
      "Current learning_rate :  9.68738e-08\n",
      "Loss :  610.406\n",
      "456 0.93359375\n",
      "Current learning_rate :  9.58585e-08\n",
      "Loss :  575.694\n",
      "457 0.9140625\n",
      "Current learning_rate :  9.48539e-08\n",
      "Loss :  569.07\n",
      "458 0.91796875\n",
      "Current learning_rate :  9.38597e-08\n",
      "Loss :  592.423\n",
      "459 0.93359375\n",
      "Current learning_rate :  9.2876e-08\n",
      "Loss :  587.382\n",
      "460 0.90625\n",
      "Current learning_rate :  9.19025e-08\n",
      "Loss :  590.589\n",
      "461 0.8984375\n",
      "Current learning_rate :  9.09394e-08\n",
      "Loss :  615.865\n",
      "462 0.90234375\n",
      "Current learning_rate :  8.99863e-08\n",
      "Loss :  585.799\n",
      "463 0.90234375\n",
      "Current learning_rate :  8.90431e-08\n",
      "Loss :  578.686\n",
      "464 0.91796875\n",
      "Current learning_rate :  8.81099e-08\n",
      "Loss :  610.091\n",
      "465 0.921875\n",
      "Current learning_rate :  8.71864e-08\n",
      "Loss :  595.193\n",
      "466 0.9140625\n",
      "Current learning_rate :  8.62727e-08\n",
      "Loss :  595.299\n",
      "467 0.921875\n",
      "Current learning_rate :  8.53685e-08\n",
      "Loss :  591.059\n",
      "468 0.94140625\n",
      "Current learning_rate :  8.44737e-08\n",
      "Loss :  603.954\n",
      "469 0.94140625\n",
      "Current learning_rate :  8.35884e-08\n",
      "Loss :  626.314\n",
      "470 0.91015625\n",
      "Current learning_rate :  8.27123e-08\n",
      "Loss :  597.592\n",
      "471 0.90625\n",
      "Current learning_rate :  8.18454e-08\n",
      "Loss :  587.365\n",
      "472 0.9296875\n",
      "Current learning_rate :  8.09876e-08\n",
      "Loss :  586.952\n",
      "473 0.92578125\n",
      "Current learning_rate :  8.01388e-08\n",
      "Loss :  573.509\n",
      "474 0.91015625\n",
      "Current learning_rate :  7.92989e-08\n",
      "Loss :  571.296\n",
      "475 0.94140625\n",
      "Current learning_rate :  7.84678e-08\n",
      "Loss :  595.688\n",
      "476 0.88671875\n",
      "Current learning_rate :  7.76454e-08\n",
      "Loss :  583.848\n",
      "477 0.8984375\n",
      "Current learning_rate :  7.68316e-08\n",
      "Loss :  598.571\n",
      "478 0.91015625\n",
      "Current learning_rate :  7.60263e-08\n",
      "Loss :  584.007\n",
      "479 0.89453125\n",
      "Current learning_rate :  7.52295e-08\n",
      "Loss :  576.419\n",
      "480 0.890625\n",
      "Current learning_rate :  7.44411e-08\n",
      "Loss :  578.716\n",
      "481 0.92578125\n",
      "Current learning_rate :  7.36609e-08\n",
      "Loss :  594.817\n",
      "482 0.890625\n",
      "Current learning_rate :  7.28889e-08\n",
      "Loss :  575.982\n",
      "483 0.9140625\n",
      "Current learning_rate :  7.21249e-08\n",
      "Loss :  597.692\n",
      "484 0.94921875\n",
      "Current learning_rate :  7.1369e-08\n",
      "Loss :  584.078\n",
      "485 0.921875\n",
      "Current learning_rate :  7.0621e-08\n",
      "Loss :  564.784\n",
      "486 0.9375\n",
      "Current learning_rate :  6.98808e-08\n",
      "Loss :  578.354\n",
      "487 0.9453125\n",
      "Current learning_rate :  6.91485e-08\n",
      "Loss :  571.51\n",
      "488 0.90234375\n",
      "Current learning_rate :  6.84237e-08\n",
      "Loss :  587.613\n",
      "489 0.9453125\n",
      "Current learning_rate :  6.77066e-08\n",
      "Loss :  584.592\n",
      "490 0.91015625\n",
      "Current learning_rate :  6.6997e-08\n",
      "Loss :  600.307\n",
      "491 0.96875\n",
      "Best model saved in file: best_model.ckpt\n",
      "Current learning_rate :  6.62948e-08\n",
      "Loss :  585.362\n",
      "492 0.91015625\n",
      "Current learning_rate :  6.56e-08\n",
      "Loss :  586.104\n",
      "493 0.9296875\n",
      "Current learning_rate :  6.49124e-08\n",
      "Loss :  605.372\n",
      "494 0.921875\n",
      "Current learning_rate :  6.42321e-08\n",
      "Loss :  583.808\n",
      "495 0.921875\n",
      "Current learning_rate :  6.35589e-08\n",
      "Loss :  585.496\n",
      "496 0.91015625\n",
      "Current learning_rate :  6.28928e-08\n",
      "Loss :  570.904\n",
      "497 0.890625\n",
      "Current learning_rate :  6.22336e-08\n",
      "Loss :  580.759\n",
      "498 0.91015625\n",
      "Current learning_rate :  6.15813e-08\n",
      "Loss :  593.242\n",
      "499 0.93359375\n",
      "Current learning_rate :  6.09359e-08\n",
      "Loss :  581.262\n",
      "500 0.94921875\n",
      "Current learning_rate :  6.02973e-08\n",
      "Loss :  599.339\n",
      "501 0.91796875\n",
      "Current learning_rate :  5.96653e-08\n",
      "Loss :  578.21\n",
      "502 0.921875\n",
      "Current learning_rate :  5.904e-08\n",
      "Loss :  579.019\n",
      "503 0.9296875\n",
      "Current learning_rate :  5.84212e-08\n",
      "Loss :  563.558\n",
      "504 0.89453125\n",
      "Current learning_rate :  5.78089e-08\n",
      "Loss :  564.097\n",
      "505 0.91015625\n",
      "Current learning_rate :  5.7203e-08\n",
      "Loss :  582.136\n",
      "506 0.91015625\n",
      "Current learning_rate :  5.66035e-08\n",
      "Loss :  601.129\n",
      "507 0.89453125\n",
      "Current learning_rate :  5.60102e-08\n",
      "Loss :  578.214\n",
      "508 0.921875\n",
      "Current learning_rate :  5.54232e-08\n",
      "Loss :  575.75\n",
      "509 0.921875\n",
      "Current learning_rate :  5.48423e-08\n",
      "Loss :  597.719\n",
      "510 0.890625\n",
      "Current learning_rate :  5.42675e-08\n",
      "Loss :  581.828\n",
      "511 0.9375\n",
      "Current learning_rate :  5.36988e-08\n",
      "Loss :  599.305\n",
      "512 0.91015625\n",
      "Current learning_rate :  5.3136e-08\n",
      "Loss :  581.244\n",
      "513 0.92578125\n",
      "Current learning_rate :  5.2579e-08\n",
      "Loss :  578.6\n",
      "514 0.92578125\n",
      "Current learning_rate :  5.2028e-08\n",
      "Loss :  594.76\n",
      "515 0.88671875\n",
      "Current learning_rate :  5.14827e-08\n",
      "Loss :  569.514\n",
      "516 0.93359375\n",
      "Current learning_rate :  5.09431e-08\n",
      "Loss :  599.963\n",
      "517 0.94140625\n",
      "Current learning_rate :  5.04092e-08\n",
      "Loss :  578.631\n",
      "518 0.91796875\n",
      "Current learning_rate :  4.98809e-08\n",
      "Loss :  609.691\n",
      "519 0.91015625\n",
      "Current learning_rate :  4.93581e-08\n",
      "Loss :  576.084\n",
      "520 0.91796875\n",
      "Current learning_rate :  4.88408e-08\n",
      "Loss :  617.851\n",
      "521 0.90234375\n",
      "Current learning_rate :  4.83289e-08\n",
      "Loss :  593.159\n",
      "522 0.91015625\n",
      "Current learning_rate :  4.78224e-08\n",
      "Loss :  578.755\n",
      "523 0.921875\n",
      "Current learning_rate :  4.73211e-08\n",
      "Loss :  589.459\n",
      "524 0.921875\n",
      "Current learning_rate :  4.68252e-08\n",
      "Loss :  572.309\n",
      "525 0.890625\n",
      "Current learning_rate :  4.63344e-08\n",
      "Loss :  596.699\n",
      "526 0.94140625\n",
      "Current learning_rate :  4.58488e-08\n",
      "Loss :  605.398\n",
      "527 0.8984375\n",
      "Current learning_rate :  4.53683e-08\n",
      "Loss :  562.165\n",
      "528 0.921875\n",
      "Current learning_rate :  4.48928e-08\n",
      "Loss :  567.09\n",
      "529 0.8984375\n",
      "Current learning_rate :  4.44223e-08\n",
      "Loss :  591.128\n",
      "530 0.91015625\n",
      "Current learning_rate :  4.39567e-08\n",
      "Loss :  591.183\n",
      "531 0.91015625\n",
      "Current learning_rate :  4.3496e-08\n",
      "Loss :  571.005\n",
      "532 0.92578125\n",
      "Current learning_rate :  4.30401e-08\n",
      "Loss :  579.359\n",
      "533 0.90234375\n",
      "Current learning_rate :  4.2589e-08\n",
      "Loss :  581.506\n",
      "534 0.9140625\n",
      "Current learning_rate :  4.21427e-08\n",
      "Loss :  618.92\n",
      "535 0.91015625\n",
      "Current learning_rate :  4.1701e-08\n",
      "Loss :  588.925\n",
      "536 0.890625\n",
      "Current learning_rate :  4.12639e-08\n",
      "Loss :  598.38\n",
      "537 0.8984375\n",
      "Current learning_rate :  4.08315e-08\n",
      "Loss :  596.294\n",
      "538 0.9140625\n",
      "Current learning_rate :  4.04035e-08\n",
      "Loss :  572.116\n",
      "539 0.89453125\n",
      "Current learning_rate :  3.998e-08\n",
      "Loss :  591.396\n",
      "540 0.8984375\n",
      "Current learning_rate :  3.9561e-08\n",
      "Loss :  585.12\n",
      "541 0.90625\n",
      "Current learning_rate :  3.91464e-08\n",
      "Loss :  566.662\n",
      "542 0.90625\n",
      "Current learning_rate :  3.87361e-08\n",
      "Loss :  569.455\n",
      "543 0.9453125\n",
      "Current learning_rate :  3.83301e-08\n",
      "Loss :  568.437\n",
      "544 0.9296875\n",
      "Current learning_rate :  3.79284e-08\n",
      "Loss :  577.938\n",
      "545 0.9296875\n",
      "Current learning_rate :  3.75309e-08\n",
      "Loss :  592.203\n",
      "546 0.90234375\n",
      "Current learning_rate :  3.71375e-08\n",
      "Loss :  584.703\n",
      "547 0.91015625\n",
      "Current learning_rate :  3.67483e-08\n",
      "Loss :  584.199\n",
      "548 0.91015625\n",
      "Current learning_rate :  3.63631e-08\n",
      "Loss :  603.926\n",
      "549 0.921875\n",
      "Current learning_rate :  3.5982e-08\n",
      "Loss :  567.325\n",
      "550 0.94140625\n",
      "Current learning_rate :  3.56049e-08\n",
      "Loss :  596.615\n",
      "551 0.91796875\n",
      "Current learning_rate :  3.52318e-08\n",
      "Loss :  578.62\n",
      "552 0.921875\n",
      "Current learning_rate :  3.48625e-08\n",
      "Loss :  608.297\n",
      "553 0.921875\n",
      "Current learning_rate :  3.44971e-08\n",
      "Loss :  588.794\n",
      "554 0.9375\n",
      "Current learning_rate :  3.41356e-08\n",
      "Loss :  588.525\n",
      "555 0.90625\n",
      "Current learning_rate :  3.37778e-08\n",
      "Loss :  584.997\n",
      "556 0.90625\n",
      "Current learning_rate :  3.34238e-08\n",
      "Loss :  596.823\n",
      "557 0.9296875\n",
      "Current learning_rate :  3.30735e-08\n",
      "Loss :  576.859\n",
      "558 0.94140625\n",
      "Current learning_rate :  3.27268e-08\n",
      "Loss :  579.86\n",
      "559 0.91015625\n",
      "Current learning_rate :  3.23838e-08\n",
      "Loss :  620.608\n",
      "560 0.890625\n",
      "Current learning_rate :  3.20444e-08\n",
      "Loss :  595.96\n",
      "561 0.9140625\n",
      "Current learning_rate :  3.17086e-08\n",
      "Loss :  602.707\n",
      "562 0.91796875\n",
      "Current learning_rate :  3.13763e-08\n",
      "Loss :  580.913\n",
      "563 0.9375\n",
      "Current learning_rate :  3.10474e-08\n",
      "Loss :  564.435\n",
      "564 0.91015625\n",
      "Current learning_rate :  3.0722e-08\n",
      "Loss :  599.711\n",
      "565 0.94140625\n",
      "Current learning_rate :  3.04e-08\n",
      "Loss :  587.833\n",
      "566 0.8984375\n",
      "Current learning_rate :  3.00814e-08\n",
      "Loss :  608.153\n",
      "567 0.90234375\n",
      "Current learning_rate :  2.97661e-08\n",
      "Loss :  591.641\n",
      "568 0.90625\n",
      "Current learning_rate :  2.94541e-08\n",
      "Loss :  569.967\n",
      "569 0.9140625\n",
      "Current learning_rate :  2.91454e-08\n",
      "Loss :  608.475\n",
      "570 0.90625\n",
      "Current learning_rate :  2.884e-08\n",
      "Loss :  566.097\n",
      "571 0.92578125\n",
      "Current learning_rate :  2.85377e-08\n",
      "Loss :  564.064\n",
      "572 0.9140625\n",
      "Current learning_rate :  2.82386e-08\n",
      "Loss :  573.188\n",
      "573 0.90625\n",
      "Current learning_rate :  2.79427e-08\n",
      "Loss :  568.206\n",
      "574 0.9140625\n",
      "Current learning_rate :  2.76498e-08\n",
      "Loss :  576.41\n",
      "575 0.890625\n",
      "Current learning_rate :  2.736e-08\n",
      "Loss :  580.366\n",
      "576 0.89453125\n",
      "Current learning_rate :  2.70733e-08\n",
      "Loss :  584.938\n",
      "577 0.93359375\n",
      "Current learning_rate :  2.67895e-08\n",
      "Loss :  590.55\n",
      "578 0.92578125\n",
      "Current learning_rate :  2.65087e-08\n",
      "Loss :  604.073\n",
      "579 0.90234375\n",
      "Current learning_rate :  2.62309e-08\n",
      "Loss :  592.38\n",
      "580 0.9140625\n",
      "Current learning_rate :  2.5956e-08\n",
      "Loss :  602.573\n",
      "581 0.93359375\n",
      "Current learning_rate :  2.5684e-08\n",
      "Loss :  592.061\n",
      "582 0.91796875\n",
      "Current learning_rate :  2.54148e-08\n",
      "Loss :  592.951\n",
      "583 0.91796875\n",
      "Current learning_rate :  2.51484e-08\n",
      "Loss :  576.225\n",
      "584 0.93359375\n",
      "Current learning_rate :  2.48848e-08\n",
      "Loss :  611.883\n",
      "585 0.91015625\n",
      "Current learning_rate :  2.4624e-08\n",
      "Loss :  606.534\n",
      "586 0.9453125\n",
      "Current learning_rate :  2.43659e-08\n",
      "Loss :  619.17\n",
      "587 0.9140625\n",
      "Current learning_rate :  2.41106e-08\n",
      "Loss :  584.084\n",
      "588 0.90234375\n",
      "Current learning_rate :  2.38579e-08\n",
      "Loss :  575.107\n",
      "589 0.93359375\n",
      "Current learning_rate :  2.36078e-08\n",
      "Loss :  573.697\n",
      "590 0.9140625\n",
      "Current learning_rate :  2.33604e-08\n",
      "Loss :  574.447\n",
      "591 0.9296875\n",
      "Current learning_rate :  2.31156e-08\n",
      "Loss :  605.722\n",
      "592 0.921875\n",
      "Current learning_rate :  2.28733e-08\n",
      "Loss :  580.869\n",
      "593 0.91796875\n",
      "Current learning_rate :  2.26336e-08\n",
      "Loss :  569.152\n",
      "594 0.8671875\n",
      "Current learning_rate :  2.23963e-08\n",
      "Loss :  588.553\n",
      "595 0.92578125\n",
      "Current learning_rate :  2.21616e-08\n",
      "Loss :  574.647\n",
      "596 0.91015625\n",
      "Current learning_rate :  2.19293e-08\n",
      "Loss :  582.018\n",
      "597 0.9296875\n",
      "Current learning_rate :  2.16995e-08\n",
      "Loss :  601.829\n",
      "598 0.91796875\n",
      "Current learning_rate :  2.14721e-08\n",
      "Loss :  601.556\n",
      "599 0.9453125\n",
      "Current learning_rate :  2.1247e-08\n",
      "Loss :  589.763\n",
      "600 0.91015625\n",
      "Current learning_rate :  2.10243e-08\n",
      "Loss :  585.88\n",
      "601 0.91015625\n",
      "Current learning_rate :  2.0804e-08\n",
      "Loss :  576.638\n",
      "602 0.9296875\n",
      "Current learning_rate :  2.0586e-08\n",
      "Loss :  587.069\n",
      "603 0.95703125\n",
      "Current learning_rate :  2.03702e-08\n",
      "Loss :  580.862\n",
      "604 0.89453125\n",
      "Current learning_rate :  2.01567e-08\n",
      "Loss :  588.014\n",
      "605 0.890625\n",
      "Current learning_rate :  1.99454e-08\n",
      "Loss :  596.46\n",
      "606 0.91796875\n",
      "Current learning_rate :  1.97364e-08\n",
      "Loss :  607.936\n",
      "607 0.9296875\n",
      "Current learning_rate :  1.95296e-08\n",
      "Loss :  580.727\n",
      "608 0.9140625\n",
      "Current learning_rate :  1.93249e-08\n",
      "Loss :  601.493\n",
      "609 0.9375\n",
      "Current learning_rate :  1.91223e-08\n",
      "Loss :  585.308\n",
      "610 0.90625\n",
      "Current learning_rate :  1.89219e-08\n",
      "Loss :  575.943\n",
      "611 0.8984375\n",
      "Current learning_rate :  1.87236e-08\n",
      "Loss :  579.95\n",
      "612 0.87890625\n",
      "Current learning_rate :  1.85274e-08\n",
      "Loss :  584.63\n",
      "613 0.921875\n",
      "Current learning_rate :  1.83332e-08\n",
      "Loss :  568.526\n",
      "614 0.9296875\n",
      "Current learning_rate :  1.8141e-08\n",
      "Loss :  579.954\n",
      "615 0.92578125\n",
      "Current learning_rate :  1.79509e-08\n",
      "Loss :  603.98\n",
      "616 0.9375\n",
      "Current learning_rate :  1.77628e-08\n",
      "Loss :  592.202\n",
      "617 0.921875\n",
      "Current learning_rate :  1.75766e-08\n",
      "Loss :  580.562\n",
      "618 0.890625\n",
      "Current learning_rate :  1.73924e-08\n",
      "Loss :  584.143\n",
      "619 0.91796875\n",
      "Current learning_rate :  1.72101e-08\n",
      "Loss :  593.783\n",
      "620 0.953125\n",
      "Current learning_rate :  1.70297e-08\n",
      "Loss :  576.739\n",
      "621 0.9375\n",
      "Current learning_rate :  1.68512e-08\n",
      "Loss :  575.527\n",
      "622 0.8984375\n",
      "Current learning_rate :  1.66746e-08\n",
      "Loss :  597.42\n",
      "623 0.90625\n",
      "Current learning_rate :  1.64999e-08\n",
      "Loss :  579.697\n",
      "624 0.91796875\n",
      "Current learning_rate :  1.63269e-08\n",
      "Loss :  585.836\n",
      "625 0.90234375\n",
      "Current learning_rate :  1.61558e-08\n",
      "Loss :  571.707\n",
      "626 0.921875\n",
      "Current learning_rate :  1.59865e-08\n",
      "Loss :  612.606\n",
      "627 0.9140625\n",
      "Current learning_rate :  1.58189e-08\n",
      "Loss :  576.522\n",
      "628 0.91015625\n",
      "Current learning_rate :  1.56531e-08\n",
      "Loss :  575.995\n",
      "629 0.8984375\n",
      "Current learning_rate :  1.54891e-08\n",
      "Loss :  607.2\n",
      "630 0.9375\n",
      "Current learning_rate :  1.53267e-08\n",
      "Loss :  596.599\n",
      "631 0.91796875\n",
      "Current learning_rate :  1.51661e-08\n",
      "Loss :  620.19\n",
      "632 0.90625\n",
      "Current learning_rate :  1.50072e-08\n",
      "Loss :  579.957\n",
      "633 0.9140625\n",
      "Current learning_rate :  1.48499e-08\n",
      "Loss :  580.995\n",
      "634 0.94140625\n",
      "Current learning_rate :  1.46942e-08\n",
      "Loss :  586.486\n",
      "635 0.9140625\n",
      "Current learning_rate :  1.45402e-08\n",
      "Loss :  568.917\n",
      "636 0.9296875\n",
      "Current learning_rate :  1.43878e-08\n",
      "Loss :  593.337\n",
      "637 0.94140625\n",
      "Current learning_rate :  1.4237e-08\n",
      "Loss :  601.569\n",
      "638 0.88671875\n",
      "Current learning_rate :  1.40878e-08\n",
      "Loss :  591.892\n",
      "639 0.9296875\n",
      "Current learning_rate :  1.39402e-08\n",
      "Loss :  590.453\n",
      "640 0.90625\n",
      "Current learning_rate :  1.37941e-08\n",
      "Loss :  614.385\n",
      "641 0.92578125\n",
      "Current learning_rate :  1.36495e-08\n",
      "Loss :  584.019\n",
      "642 0.90625\n",
      "Current learning_rate :  1.35064e-08\n",
      "Loss :  570.764\n",
      "643 0.9296875\n",
      "Current learning_rate :  1.33649e-08\n",
      "Loss :  587.418\n",
      "644 0.9140625\n",
      "Current learning_rate :  1.32248e-08\n",
      "Loss :  597.139\n",
      "645 0.91015625\n",
      "Current learning_rate :  1.30862e-08\n",
      "Loss :  583.947\n",
      "646 0.8984375\n",
      "Current learning_rate :  1.29491e-08\n",
      "Loss :  582.895\n",
      "647 0.921875\n",
      "Current learning_rate :  1.28133e-08\n",
      "Loss :  597.425\n",
      "648 0.89453125\n",
      "Current learning_rate :  1.2679e-08\n",
      "Loss :  582.925\n",
      "649 0.9140625\n",
      "Current learning_rate :  1.25462e-08\n",
      "Loss :  601.667\n",
      "650 0.9296875\n",
      "Current learning_rate :  1.24147e-08\n",
      "Loss :  592.338\n",
      "651 0.8984375\n",
      "Current learning_rate :  1.22846e-08\n",
      "Loss :  575.802\n",
      "652 0.90234375\n",
      "Current learning_rate :  1.21558e-08\n",
      "Loss :  572.818\n",
      "653 0.91796875\n",
      "Current learning_rate :  1.20284e-08\n",
      "Loss :  597.792\n",
      "654 0.90234375\n",
      "Current learning_rate :  1.19023e-08\n",
      "Loss :  597.971\n",
      "655 0.90234375\n",
      "Current learning_rate :  1.17776e-08\n",
      "Loss :  608.762\n",
      "656 0.921875\n",
      "Current learning_rate :  1.16542e-08\n",
      "Loss :  574.345\n",
      "657 0.91796875\n",
      "Current learning_rate :  1.1532e-08\n",
      "Loss :  612.934\n",
      "658 0.8828125\n",
      "Current learning_rate :  1.14111e-08\n",
      "Loss :  602.782\n",
      "659 0.8828125\n",
      "Current learning_rate :  1.12915e-08\n",
      "Loss :  578.256\n",
      "660 0.90234375\n",
      "Current learning_rate :  1.11732e-08\n",
      "Loss :  597.861\n",
      "661 0.93359375\n",
      "Current learning_rate :  1.10561e-08\n",
      "Loss :  613.315\n",
      "662 0.91796875\n",
      "Current learning_rate :  1.09402e-08\n",
      "Loss :  620.198\n",
      "663 0.90234375\n",
      "Current learning_rate :  1.08256e-08\n",
      "Loss :  595.573\n",
      "664 0.93359375\n",
      "Current learning_rate :  1.07121e-08\n",
      "Loss :  597.075\n",
      "665 0.91796875\n",
      "Current learning_rate :  1.05998e-08\n",
      "Loss :  590.687\n",
      "666 0.91796875\n",
      "Current learning_rate :  1.04887e-08\n",
      "Loss :  593.997\n",
      "667 0.9140625\n",
      "Current learning_rate :  1.03788e-08\n",
      "Loss :  580.043\n",
      "668 0.90234375\n",
      "Current learning_rate :  1.027e-08\n",
      "Loss :  591.073\n",
      "669 0.91015625\n",
      "Current learning_rate :  1.01624e-08\n",
      "Loss :  572.833\n",
      "670 0.91796875\n",
      "Current learning_rate :  1.00559e-08\n",
      "Loss :  594.555\n",
      "671 0.91796875\n",
      "Current learning_rate :  9.95049e-09\n",
      "Loss :  588.119\n",
      "672 0.92578125\n",
      "Current learning_rate :  9.8462e-09\n",
      "Loss :  592.486\n",
      "673 0.90625\n",
      "Current learning_rate :  9.743e-09\n",
      "Loss :  566.721\n",
      "674 0.921875\n",
      "Current learning_rate :  9.64089e-09\n",
      "Loss :  597.676\n",
      "675 0.90625\n",
      "Current learning_rate :  9.53984e-09\n",
      "Loss :  584.428\n",
      "676 0.9140625\n",
      "Current learning_rate :  9.43986e-09\n",
      "Loss :  578.052\n",
      "677 0.9296875\n",
      "Current learning_rate :  9.34093e-09\n",
      "Loss :  576.218\n",
      "678 0.90625\n",
      "Current learning_rate :  9.24302e-09\n",
      "Loss :  571.292\n",
      "679 0.8828125\n",
      "Current learning_rate :  9.14615e-09\n",
      "Loss :  582.003\n",
      "680 0.93359375\n",
      "Current learning_rate :  9.05029e-09\n",
      "Loss :  559.358\n",
      "681 0.9140625\n",
      "Current learning_rate :  8.95544e-09\n",
      "Loss :  593.355\n",
      "682 0.8828125\n",
      "Current learning_rate :  8.86158e-09\n",
      "Loss :  594.524\n",
      "683 0.90625\n",
      "Current learning_rate :  8.7687e-09\n",
      "Loss :  597.915\n",
      "684 0.91015625\n",
      "Current learning_rate :  8.6768e-09\n",
      "Loss :  582.596\n",
      "685 0.91796875\n",
      "Current learning_rate :  8.58586e-09\n",
      "Loss :  590.452\n",
      "686 0.9296875\n",
      "Current learning_rate :  8.49588e-09\n",
      "Loss :  614.663\n",
      "687 0.92578125\n",
      "Current learning_rate :  8.40683e-09\n",
      "Loss :  583.617\n",
      "688 0.89453125\n",
      "Current learning_rate :  8.31872e-09\n",
      "Loss :  576.079\n",
      "689 0.9296875\n",
      "Current learning_rate :  8.23153e-09\n",
      "Loss :  610.081\n",
      "690 0.9375\n",
      "Current learning_rate :  8.14526e-09\n",
      "Loss :  617.476\n",
      "691 0.89453125\n",
      "Current learning_rate :  8.05989e-09\n",
      "Loss :  570.882\n",
      "692 0.95703125\n",
      "Current learning_rate :  7.97542e-09\n",
      "Loss :  583.28\n",
      "693 0.953125\n",
      "Current learning_rate :  7.89183e-09\n",
      "Loss :  581.032\n",
      "694 0.9140625\n",
      "Current learning_rate :  7.80912e-09\n",
      "Loss :  592.399\n",
      "695 0.953125\n",
      "Current learning_rate :  7.72727e-09\n",
      "Loss :  560.651\n",
      "696 0.89453125\n",
      "Current learning_rate :  7.64629e-09\n",
      "Loss :  581.448\n",
      "697 0.9296875\n",
      "Current learning_rate :  7.56615e-09\n",
      "Loss :  593.914\n",
      "698 0.90625\n",
      "Current learning_rate :  7.48685e-09\n",
      "Loss :  584.127\n",
      "699 0.91015625\n",
      "Current learning_rate :  7.40838e-09\n",
      "Loss :  593.654\n",
      "700 0.8984375\n",
      "Current learning_rate :  7.33074e-09\n",
      "Loss :  635.184\n",
      "701 0.94921875\n",
      "Current learning_rate :  7.25391e-09\n",
      "Loss :  568.462\n",
      "702 0.921875\n",
      "Current learning_rate :  7.17788e-09\n",
      "Loss :  599.586\n",
      "703 0.9140625\n",
      "Current learning_rate :  7.10265e-09\n",
      "Loss :  577.607\n",
      "704 0.9296875\n",
      "Current learning_rate :  7.02821e-09\n",
      "Loss :  591.752\n",
      "705 0.92578125\n",
      "Current learning_rate :  6.95455e-09\n",
      "Loss :  590.373\n",
      "706 0.91796875\n",
      "Current learning_rate :  6.88166e-09\n",
      "Loss :  585.486\n",
      "707 0.92578125\n",
      "Current learning_rate :  6.80953e-09\n",
      "Loss :  598.204\n",
      "708 0.91796875\n",
      "Current learning_rate :  6.73816e-09\n",
      "Loss :  554.554\n",
      "709 0.9140625\n",
      "Current learning_rate :  6.66754e-09\n",
      "Loss :  587.448\n",
      "710 0.90625\n",
      "Current learning_rate :  6.59766e-09\n",
      "Loss :  575.716\n",
      "711 0.9296875\n",
      "Current learning_rate :  6.52851e-09\n",
      "Loss :  590.396\n",
      "712 0.921875\n",
      "Current learning_rate :  6.46009e-09\n",
      "Loss :  593.507\n",
      "713 0.92578125\n",
      "Current learning_rate :  6.39238e-09\n",
      "Loss :  555.433\n",
      "714 0.92578125\n",
      "Current learning_rate :  6.32539e-09\n",
      "Loss :  579.848\n",
      "715 0.921875\n",
      "Current learning_rate :  6.25909e-09\n",
      "Loss :  572.852\n",
      "716 0.921875\n",
      "Current learning_rate :  6.19349e-09\n",
      "Loss :  579.973\n",
      "717 0.90625\n",
      "Current learning_rate :  6.12858e-09\n",
      "Loss :  590.02\n",
      "718 0.92578125\n",
      "Current learning_rate :  6.06435e-09\n",
      "Loss :  577.565\n",
      "719 0.91015625\n",
      "Current learning_rate :  6.00079e-09\n",
      "Loss :  587.955\n",
      "720 0.90625\n",
      "Current learning_rate :  5.9379e-09\n",
      "Loss :  604.148\n",
      "721 0.9375\n",
      "Current learning_rate :  5.87566e-09\n",
      "Loss :  585.615\n",
      "722 0.9296875\n",
      "Current learning_rate :  5.81408e-09\n",
      "Loss :  581.656\n",
      "723 0.890625\n",
      "Current learning_rate :  5.75314e-09\n",
      "Loss :  600.681\n",
      "724 0.91015625\n",
      "Current learning_rate :  5.69285e-09\n",
      "Loss :  595.661\n",
      "725 0.890625\n",
      "Current learning_rate :  5.63318e-09\n",
      "Loss :  588.668\n",
      "726 0.90234375\n",
      "Current learning_rate :  5.57414e-09\n",
      "Loss :  567.837\n",
      "727 0.9140625\n",
      "Current learning_rate :  5.51572e-09\n",
      "Loss :  604.474\n",
      "728 0.9296875\n",
      "Current learning_rate :  5.45791e-09\n",
      "Loss :  583.144\n",
      "729 0.9140625\n",
      "Current learning_rate :  5.40071e-09\n",
      "Loss :  600.156\n",
      "730 0.89453125\n",
      "Current learning_rate :  5.34411e-09\n",
      "Loss :  585.977\n",
      "731 0.92578125\n",
      "Current learning_rate :  5.2881e-09\n",
      "Loss :  604.27\n",
      "732 0.921875\n",
      "Current learning_rate :  5.23267e-09\n",
      "Loss :  614.93\n",
      "733 0.8984375\n",
      "Current learning_rate :  5.17783e-09\n",
      "Loss :  572.228\n",
      "734 0.91796875\n",
      "Current learning_rate :  5.12356e-09\n",
      "Loss :  580.659\n",
      "735 0.8984375\n",
      "Current learning_rate :  5.06986e-09\n",
      "Loss :  592.814\n",
      "736 0.91796875\n",
      "Current learning_rate :  5.01673e-09\n",
      "Loss :  598.064\n",
      "737 0.92578125\n",
      "Current learning_rate :  4.96415e-09\n",
      "Loss :  572.857\n",
      "738 0.92578125\n",
      "Current learning_rate :  4.91212e-09\n",
      "Loss :  580.066\n",
      "739 0.8828125\n",
      "Current learning_rate :  4.86064e-09\n",
      "Loss :  618.253\n",
      "740 0.90234375\n",
      "Current learning_rate :  4.80969e-09\n",
      "Loss :  566.787\n",
      "741 0.9140625\n",
      "Current learning_rate :  4.75929e-09\n",
      "Loss :  589.803\n",
      "742 0.91796875\n",
      "Current learning_rate :  4.70941e-09\n",
      "Loss :  579.995\n",
      "743 0.91796875\n",
      "Current learning_rate :  4.66005e-09\n",
      "Loss :  597.64\n",
      "744 0.890625\n",
      "Current learning_rate :  4.61121e-09\n",
      "Loss :  572.893\n",
      "745 0.90234375\n",
      "Current learning_rate :  4.56288e-09\n",
      "Loss :  604.245\n",
      "746 0.9453125\n",
      "Current learning_rate :  4.51506e-09\n",
      "Loss :  568.238\n",
      "747 0.90625\n",
      "Current learning_rate :  4.46774e-09\n",
      "Loss :  580.915\n",
      "748 0.921875\n",
      "Current learning_rate :  4.42091e-09\n",
      "Loss :  605.828\n",
      "749 0.89453125\n",
      "Current learning_rate :  4.37457e-09\n",
      "Loss :  558.63\n",
      "750 0.90234375\n",
      "Current learning_rate :  4.32873e-09\n",
      "Loss :  595.436\n",
      "751 0.91796875\n",
      "Current learning_rate :  4.28336e-09\n",
      "Loss :  578.627\n",
      "752 0.921875\n",
      "Current learning_rate :  4.23847e-09\n",
      "Loss :  600.081\n",
      "753 0.9140625\n",
      "Current learning_rate :  4.19404e-09\n",
      "Loss :  591.794\n",
      "754 0.9140625\n",
      "Current learning_rate :  4.15008e-09\n",
      "Loss :  591.752\n",
      "755 0.92578125\n",
      "Current learning_rate :  4.10659e-09\n",
      "Loss :  593.03\n",
      "756 0.9296875\n",
      "Current learning_rate :  4.06355e-09\n",
      "Loss :  552.359\n",
      "757 0.90234375\n",
      "Current learning_rate :  4.02096e-09\n",
      "Loss :  585.28\n",
      "758 0.9140625\n",
      "Current learning_rate :  3.97882e-09\n",
      "Loss :  577.637\n",
      "759 0.8984375\n",
      "Current learning_rate :  3.93712e-09\n",
      "Loss :  579.315\n",
      "760 0.90625\n",
      "Interrupted\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # If we have the max model saved\n",
    "    max_acc = 0\n",
    "    try:\n",
    "        saver.restore(sess, \"best_model.ckpt\")\n",
    "        max_acc = np.mean(np.argmax(y_val[test_indices], axis=1) ==\n",
    "                             sess.run(predict_op, feed_dict={X: X_val[test_indices],\n",
    "                                                             y: y_val[test_indices],\n",
    "                                                             conv_dropout: 1.0,\n",
    "                                                             fc_dropout: 1.0,\n",
    "                                                             train_batch: False}))\n",
    "        print(\"Getting max accuracy model: \", max_acc)\n",
    "    # If we already have the model saved\n",
    "    except:\n",
    "        print(\"Was unable to read best_model.ckpt checkpoint. max_acc = 0\")\n",
    "        \n",
    "    try:\n",
    "        saver.restore(sess, \"model.ckpt\")\n",
    "        print(\"Starting from model.ckpt checkpoint.\")\n",
    "    except:\n",
    "        print(\"Was unable to read model.ckpt checkpoint.\")\n",
    "        # you need to initialize all variables\n",
    "        init_op = tf.initialize_all_variables()\n",
    "        sess.run(init_op)\n",
    "        \n",
    "    writer = tf.train.SummaryWriter('logs', sess.graph_def)\n",
    "    \n",
    "    try:    \n",
    "        for i in range(100000):\n",
    "            NUM_BATCHES = 10\n",
    "            for j in range(NUM_BATCHES):\n",
    "                batch_mask = np.random.choice(44000, 128)\n",
    "                summary_str, _, cur_loss = sess.run([loss_summary, train_op, cross_entropy], \n",
    "                                                    feed_dict={X: X_train[batch_mask], \n",
    "                                                               y: y_train[batch_mask],\n",
    "                                                               conv_dropout: 0.5, \n",
    "                                                               fc_dropout: 0.5,\n",
    "                                                               train_batch: True})\n",
    "            print(\"Current learning_rate : \", learning_rate.eval())\n",
    "            test_indices = np.arange(len(X_val)) # Get a validation batch\n",
    "            test_indices = test_indices[0:256]\n",
    "\n",
    "            writer.add_summary(summary_str, i)\n",
    "            print(\"Loss : \" , cur_loss)\n",
    "            acc = np.mean(np.argmax(y_val[test_indices], axis=1) ==\n",
    "                             sess.run(predict_op, feed_dict={X: X_val[test_indices],\n",
    "                                                             y: y_val[test_indices],\n",
    "                                                             conv_dropout: 1.0,\n",
    "                                                             fc_dropout: 1.0,\n",
    "                                                             train_batch: False}))\n",
    "            print(i, acc)\n",
    "            if acc > max_acc:\n",
    "                max_acc = acc\n",
    "                save_path = saver.save(sess, \"best_model.ckpt\") # checkpoint file\n",
    "                print(\"Best model saved in file: %s\" % save_path)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('Interrupted')\n",
    "        save_path = saver.save(sess, \"model.ckpt\") # checkpoint file\n",
    "        print(\"Model saved in file: %s\" % save_path)\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n",
    "    save_path = saver.save(sess, \"model.ckpt\") # checkpoint file\n",
    "    print(\"Model saved in file: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.94140625\n",
      "Answers :  [8 6 9 4 5 1 8 9 7 5]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAFzCAYAAAAjYj0YAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11375d748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"best_model.ckpt\")\n",
    "    \n",
    "    test_indices = np.arange(len(teX)) # Get a validation batch\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    test_indices = test_indices[0:256]\n",
    "\n",
    "    acc = np.mean(np.argmax(y_val[test_indices], axis=1) ==\n",
    "                             sess.run(predict_op, feed_dict={X: X_val[test_indices],\n",
    "                                                             y: y_val[test_indices],\n",
    "                                                             conv_dropout: 1.0,\n",
    "                                                             fc_dropout: 1.0,\n",
    "                                                             train_batch: False}))\n",
    "\n",
    "    answers = sess.run(predict_op, feed_dict={X: teX[test_indices],\n",
    "                                     y: teY[test_indices],\n",
    "                                     conv_dropout: 1.0,\n",
    "                                     fc_dropout: 1.0})\n",
    "    print(\"Accuracy : \" , acc)\n",
    "    for i in range(SHOW_IMAGE):\n",
    "        plt.subplot(SHOW_IMAGE/5, SHOW_IMAGE/2, i+1)\n",
    "        implot = plt.imshow(teX[test_indices[i]].reshape((28,28)))\n",
    "        plt.axis('off')\n",
    "    print(\"Answers : \", answers[:SHOW_IMAGE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
